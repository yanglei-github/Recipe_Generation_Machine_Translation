{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"phrasal_encoder.ipynb","provenance":[],"mount_file_id":"155lK8OwlIG6C9KDoeqlirBnsXgbdZlrT","authorship_tag":"ABX9TyPrn6DZ4FUXps0bSDCnWB0A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Gyxd91x3ql_b","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":485},"executionInfo":{"status":"ok","timestamp":1599131741923,"user_tz":-480,"elapsed":19649,"user":{"displayName":"lei ya","photoUrl":"","userId":"05028726856435333308"}},"outputId":"1862c3fb-22fb-4fd5-a8e5-5aa7d97698f4"},"source":["from nltk.corpus import stopwords\n","import tensorflow as tf\n","import matplotlib as mpl\n","import matplotlib.pyplot as  plt\n","import numpy as np\n","import pandas as pd\n","import sklearn\n","import os\n","import sys\n","import time\n","from tensorflow import keras\n","\n","import tensorflow as tf\n","#fitting for gpu\n","\n","\n","config = tf.compat.v1.ConfigProto()\n","config.gpu_options.allow_growth = True\n","session = tf.compat.v1.Session(config=config)\n","\n","df_clean = pd.read_csv(\"./drive/My Drive/total_data_withcommainput.csv\")\n","#---------------------------------------------------------------------------------------\n","given_ingredient = list(df_clean.ingredients)[0:192]\n","#-----------------------------创建评估用词表--------------------------------------------\n","reference_recipe = list(df_clean.recipes)\n","reference_meteor = []\n","references = []\n","reference_list = reference_recipe[0:192]\n","for astr in reference_list:\n","    alist = astr.split(' ')[1:-1]\n","    reference_meteor.append(' '.join(alist))\n","    references.append([alist])\n","clean_summaries = list(df_clean.ingredients)\n","clean_texts = list(df_clean.recipes)\n","sp_dataset = tuple(clean_summaries)\n","en_dataset = tuple(clean_texts)\n","#将词语式数据转成ID式\n","def tokenizer(lang):\n","    lang_tokenizer = keras.preprocessing.text.Tokenizer(\n","        num_words=None, filters='', split=' ')\n","    lang_tokenizer.fit_on_texts(lang)#统计词频，生成词表\n","    tensor = lang_tokenizer.texts_to_sequences(lang)\n","    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding='post')#在句子后面做padding\n","    return tensor, lang_tokenizer\n","\n","def tokenizer1(lang):\n","    lang_tokenizer = keras.preprocessing.text.Tokenizer(\n","        num_words=None, filters='', split=',')\n","    lang_tokenizer.fit_on_texts(lang)#统计词频，生成词表\n","    tensor = lang_tokenizer.texts_to_sequences(lang)\n","    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding='post')#在句子后面做padding\n","    return tensor, lang_tokenizer\n","\n","input_tensor, input_tokenizer = tokenizer1(sp_dataset)\n","output_tensor, output_tokenizer = tokenizer(en_dataset)\n","\n","def max_length(tensor):\n","    return max(len(t) for t in tensor)\n","\n","max_length_input = max_length(input_tensor)\n","max_length_output = max_length(output_tensor)\n","print(max_length_input, max_length_output)\n","\n","from sklearn.model_selection import train_test_split\n","input_train, input_eval, output_train, output_eval = train_test_split(input_tensor[192:], output_tensor[192:], test_size=0.01, shuffle=False)\n","\n","batch_size = 64\n","epochs = 10\n","dataset = tf.data.Dataset.from_tensor_slices((input_train, output_train))\n","train_dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder= True)\n","train_dataset = train_dataset.shuffle(10000)\n","eval_dataset = tf.data.Dataset.from_tensor_slices((input_eval, output_eval))\n","eval_dataset = eval_dataset.repeat(epochs).batch(batch_size, drop_remainder= True)\n","\n","for x, y in train_dataset.take(1):\n","    print(x.shape)\n","    print(y.shape)\n","    print(x)\n","    print(y)\n","\n","embedding_units = 256\n","units = 256\n","#input_tokenizer.word_index是个字典\n","input_vocab_size = len(input_tokenizer.word_index) + 1\n","output_vocab_size = len(output_tokenizer.word_index) + 1\n","print(input_vocab_size,output_vocab_size)\n","#-------------------lstm-----------------------------------\n","class Encoder(keras.Model):\n","    def __init__(self, vocab_size, embedding_units, encoding_units, batch_size):\n","        super(Encoder, self).__init__()\n","        self.batch_size = batch_size\n","        self.encoding_units = encoding_units\n","        self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n","        #由于用attention，每步输出需要return_sequences = True\n","        self.gru = keras.layers.LSTM(self.encoding_units,\n","                                   return_sequences = True,\n","                                   return_state = True,\n","                                   recurrent_initializer = 'glorot_uniform')\n","        \n","        \n","    #hidden是初始化的隐含状态\n","    #这里hidden应该变成[hiddenstate,hiddenc]\n","    def call(self, x, hidden):\n","        x = self.embedding(x)\n","        output, state, state_c = self.gru(x, initial_state = hidden)\n","        #output = keras.layers.Dropout(0.75)(output)\n","        return output, state, state_c\n","    \n","    def initialize_hidden_state(self):\n","        return [tf.zeros((self.batch_size, self.encoding_units)),tf.zeros((self.batch_size, self.encoding_units))]\n","    \n","encoder = Encoder(input_vocab_size, embedding_units, units, batch_size)\n","sample_hidden = encoder.initialize_hidden_state()\n","sample_output, sample_hidden, sample_c = encoder.call(x, sample_hidden)\n","\n","print('sample_output.shape: ', sample_output.shape)\n","print('sample_hidden.shape: ', sample_hidden.shape)\n","print('sample_memory.shape: ', sample_c.shape)\n","\n","\n","class BahdanauAttention(keras.Model):\n","    \n","    def __init__(self, units):\n","        super(BahdanauAttention, self).__init__()\n","        self.W1 = keras.layers.Dense(units)\n","        self.W2 = keras.layers.Dense(units)\n","        self.V = keras.layers.Dense(1)\n","        \n","    def call(self, decoder_hidden, encoder_outputs):\n","        #decoder_hidden.shape = (batch_size, units)\n","        #encoder_outputs.shape = (batch_size, length, units)\n","        decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, 1)#decoder_hidden.shape = (batch_size, 1, units)\n","        #before V:tf.nn.tanh/shape: (batch_size, length, units)\n","        #after V:(batch_size, length, 1)\n","        score = self.V(\n","            tf.nn.tanh(\n","                self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time_axis)))\n","        \n","        #shape: (batch_size, length, 1)\n","        attention_weights = tf.nn.softmax(score, axis = 1)\n","        #context_vector.shape:(batch_size, length, units)\n","        context_vector = attention_weights * encoder_outputs\n","        #context_vector.shape:(batch_size, units)\n","        context_vector = tf.reduce_sum(context_vector, axis = 1)#在length上求和\n","        \n","        return context_vector, attention_weights\n","\n","attention_model = BahdanauAttention(units = 10)#units:经过W1之后的units个数，与batch_size, length, units里units不同\n","attention_results, attention_weights = attention_model.call(sample_hidden,sample_output)\n","\n","print(attention_results.shape)\n","print(attention_weights.shape)\n","\n","#---------------------------------lstm-----------------------------------------\n","class Decoder(keras.Model):\n","    def __init__(self, vocab_size, embedding_units, decoding_units, batch_size):\n","        super(Decoder, self).__init__()#调用父类（keras.Model）的构造函数\n","        self.batch_size = batch_size\n","        self.decoding_units = decoding_units\n","        self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n","        self.gru = keras.layers.LSTM(self.decoding_units,\n","                                   return_sequences = True,\n","                                   return_state = True,\n","                                   recurrent_initializer = 'glorot_uniform')\n","        \n","        self.fc = keras.layers.Dense(vocab_size)\n","        self.attention = BahdanauAttention(self.decoding_units)\n","        \n","        #x:当前步输入，hidden:前一步输出\n","    def call(self, x, hidden, encoding_outputs,encoding_state):\n","        #context_vector.shape: (bathc_size, units)\n","        context_vector, attention_weights = self.attention.call(hidden, encoding_outputs)\n","        #befor embedding:x.shape:(batch_size, 1)\n","        #after embedding:x.shape:(batch_size, 1, embedding_units)\n","        \n","        x = self.embedding(x)\n","        \n","        combined_x = tf.concat([tf.expand_dims(context_vector, 1),x], axis = -1)\n","        #output.shape: (batch_size, 1, decoding_units)\n","        #state.shape: (batch_size, decoding_units)\n","        output, state, state_c = self.gru(combined_x, initial_state = [hidden,encoding_state])\n","        #output = keras.layers.Dropout(0.5)(output)\n","        #output.shape: (batch_size, decoding_units)\n","        output = tf.reshape(output, (-1, output.shape[2]))\n","        \n","        #output.shape: (batch_size, vocab_size)\n","        output = self.fc(output)\n","        \n","        return output, state, attention_weights, state_c\n","decoder = Decoder(output_vocab_size, embedding_units, units, batch_size)\n","\n","outputs = decoder.call(tf.random.uniform((batch_size,1)), sample_hidden, sample_output,sample_hidden)\n","\n","decoder_output, decoder_hidden, decoder_aw, state_c = outputs\n","\n","print(decoder_output.shape)\n","print(decoder_hidden.shape)\n","print(decoder_aw.shape)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["18 216\n","(64, 18)\n","(64, 216)\n","tf.Tensor(\n","[[   1  461    3 ...    0    0    0]\n"," [   1   11   23 ...    0    0    0]\n"," [   1   13 4299 ...    0    0    0]\n"," ...\n"," [   1  972  150 ...    0    0    0]\n"," [   1    4  423 ...    0    0    0]\n"," [   1  391    3 ...    0    0    0]], shape=(64, 18), dtype=int32)\n","tf.Tensor(\n","[[11  6  5 ...  0  0  0]\n"," [11 42 17 ...  0  0  0]\n"," [11 16 47 ...  0  0  0]\n"," ...\n"," [11 42 17 ...  0  0  0]\n"," [11  6  5 ...  0  0  0]\n"," [11 42 81 ...  0  0  0]], shape=(64, 216), dtype=int32)\n","15173 6756\n","sample_output.shape:  (64, 18, 256)\n","sample_hidden.shape:  (64, 256)\n","sample_memory.shape:  (64, 256)\n","(64, 256)\n","(64, 18, 1)\n","(64, 6756)\n","(64, 256)\n","(64, 18, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T52AuiycquY5","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599131741926,"user_tz":-480,"elapsed":19645,"user":{"displayName":"lei ya","photoUrl":"","userId":"05028726856435333308"}}},"source":["optimizer = keras.optimizers.Adam()\n","\n","loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = 'none')\n","#from_logits直接经过fc的输出没有经过softmax,如果经过softmax就设成False\n","\n","def loss_function(real, pred):\n","    #输出里的padding不应该计算到损失函数中去\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))#是padding时，mask取0\n","    loss_ = loss_object(real, pred)\n","    mask = tf.cast(mask, dtype = loss_.dtype)\n","    loss_ *= mask\n","    \n","    return tf.reduce_mean(loss_)\n","\n","#lstm-----------------------------------------\n","@tf.function#加速cell\n","def train_step(inp, targ, encoding_hidden):\n","    loss = 0\n","    with tf.GradientTape() as tape:\n","        encoding_outputs, encoding_hidden, encoding_c = encoder(inp, encoding_hidden)\n","        \n","        decoding_hidden = encoding_hidden\n","        decoding_state = encoding_c\n","        #eg: <start> I am here <end>\n","        #1. <start> -> I\n","        #2. I -> am\n","        #3. am -> here\n","        #4. here -> <end>\n","        for t in range(0, targ.shape[1]-1):\n","            decoding_input = tf.expand_dims(targ[:, t],1)\n","            predictions, decoding_hidden, _, decoding_state = decoder.call(decoding_input, decoding_hidden, encoding_outputs,decoding_state)\n","            loss += loss_function(targ[:, t+1],predictions)\n","            \n","    batch_loss = loss / int(targ.shape[0])\n","        \n","    variables = encoder.trainable_variables + decoder.trainable_variables\n","        \n","    gradients = tape.gradient(loss, variables)\n","    optimizer.apply_gradients(zip(gradients, variables))\n","        \n","    return batch_loss  \n","\n","#-------------------------------------------lstm-------------------------\n","def eval_step(inp, targ, encoding_hidden):\n","    loss = 0\n","    \n","    encoding_outputs, encoding_hidden, encoding_c = encoder(inp, encoding_hidden)\n","        \n","    decoding_hidden = encoding_hidden\n","    decoding_state = encoding_c\n","        #eg: <start> I am here <end>\n","        #1. <start> -> I\n","        #2. I -> am\n","        #3. am -> here\n","        #4. here -> <end>\n","    for t in range(0, targ.shape[1]-1):\n","        decoding_input = tf.expand_dims(targ[:, t],1)\n","        predictions, decoding_hidden, _, decoding_state = decoder.call(decoding_input, decoding_hidden, encoding_outputs, decoding_state)\n","        loss += loss_function(targ[:, t+1],predictions)\n","            \n","    batch_loss = loss / int(targ.shape[0])\n","    return batch_loss "],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"inQFNrBHqxDH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1599135670211,"user_tz":-480,"elapsed":3947927,"user":{"displayName":"lei ya","photoUrl":"","userId":"05028726856435333308"}},"outputId":"3339f1e6-4c20-4781-999e-1e445ab53e94"},"source":["adict = {'training_loss':[], 'validation_loss':[]}\n","alist = []\n","import time\n","#epochs = 20\n","epochs = 10\n","# steps_per_epoch = len(input_tensor[192:18973]) // batch_size\n","# steps_per_epoch1 = len(input_tensor[18973:]) // batch_size\n","steps_per_epoch = len(input_train) // batch_size\n","steps_per_epoch1 = len(input_eval) // batch_size\n","for epoch in range(epochs):\n","    start = time.time()\n","    \n","    encoding_hidden = encoder.initialize_hidden_state()\n","    total_loss = 0\n","    total_loss1 = 0\n","    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n","        \n","        batch_loss = train_step(inp, targ, encoding_hidden)\n","        total_loss += batch_loss\n","        \n","        if batch % 100 == 0:\n","            print('Epoch {} Batch {} Loss {:4f}'.format(epoch+1, batch, batch_loss.numpy()))\n","    for (batch1, (inp, targ)) in enumerate(eval_dataset.take(steps_per_epoch1)):\n","        eval_loss = eval_step(inp, targ, encoding_hidden)\n","        total_loss1 += eval_loss\n","               \n","            \n","      \n","    print('Epoch {} Loss {:4f}'.format(epoch+1, total_loss / steps_per_epoch))\n","    print('Epoch {} Eval_Loss {:4f}'.format(epoch+1, total_loss1 / steps_per_epoch1))\n","    print('Time take for 1 epoch {} sec\\n'.format(time.time() - start))\n","    adict['training_loss'].append(round(float(total_loss / steps_per_epoch),3))\n","    adict['validation_loss'].append(round(float(total_loss1 / steps_per_epoch1),3))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Epoch 1 Batch 0 Loss 11.653750\n","Epoch 1 Batch 100 Loss 6.275951\n","Epoch 1 Batch 200 Loss 7.405527\n","Epoch 1 Loss 7.461881\n","Epoch 1 Eval_Loss 7.354932\n","Time take for 1 epoch 689.0726516246796 sec\n","\n","Epoch 2 Batch 0 Loss 6.560984\n","Epoch 2 Batch 100 Loss 6.157405\n","Epoch 2 Batch 200 Loss 5.492950\n","Epoch 2 Loss 5.924222\n","Epoch 2 Eval_Loss 5.517000\n","Time take for 1 epoch 360.16733407974243 sec\n","\n","Epoch 3 Batch 0 Loss 4.917969\n","Epoch 3 Batch 100 Loss 4.717942\n","Epoch 3 Batch 200 Loss 4.413568\n","Epoch 3 Loss 4.580320\n","Epoch 3 Eval_Loss 4.683015\n","Time take for 1 epoch 358.7112581729889 sec\n","\n","Epoch 4 Batch 0 Loss 3.478528\n","Epoch 4 Batch 100 Loss 4.066831\n","Epoch 4 Batch 200 Loss 3.660504\n","Epoch 4 Loss 4.037931\n","Epoch 4 Eval_Loss 4.348739\n","Time take for 1 epoch 360.3256883621216 sec\n","\n","Epoch 5 Batch 0 Loss 3.910862\n","Epoch 5 Batch 100 Loss 4.083760\n","Epoch 5 Batch 200 Loss 3.610471\n","Epoch 5 Loss 3.721468\n","Epoch 5 Eval_Loss 4.122391\n","Time take for 1 epoch 359.14231991767883 sec\n","\n","Epoch 6 Batch 0 Loss 3.053639\n","Epoch 6 Batch 100 Loss 3.712471\n","Epoch 6 Batch 200 Loss 3.098374\n","Epoch 6 Loss 3.476355\n","Epoch 6 Eval_Loss 3.944174\n","Time take for 1 epoch 359.70510625839233 sec\n","\n","Epoch 7 Batch 0 Loss 3.006651\n","Epoch 7 Batch 100 Loss 3.151759\n","Epoch 7 Batch 200 Loss 2.855686\n","Epoch 7 Loss 3.305844\n","Epoch 7 Eval_Loss 3.850735\n","Time take for 1 epoch 360.01633977890015 sec\n","\n","Epoch 8 Batch 0 Loss 3.288687\n","Epoch 8 Batch 100 Loss 3.228290\n","Epoch 8 Batch 200 Loss 3.422707\n","Epoch 8 Loss 3.181996\n","Epoch 8 Eval_Loss 3.761499\n","Time take for 1 epoch 359.53815150260925 sec\n","\n","Epoch 9 Batch 0 Loss 3.182651\n","Epoch 9 Batch 100 Loss 3.352154\n","Epoch 9 Batch 200 Loss 3.125652\n","Epoch 9 Loss 3.051816\n","Epoch 9 Eval_Loss 3.704910\n","Time take for 1 epoch 361.5810582637787 sec\n","\n","Epoch 10 Batch 0 Loss 2.859310\n","Epoch 10 Batch 100 Loss 2.844194\n","Epoch 10 Batch 200 Loss 2.872056\n","Epoch 10 Loss 2.969047\n","Epoch 10 Eval_Loss 3.622087\n","Time take for 1 epoch 360.12920093536377 sec\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lJmNo3BSq02J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":351},"executionInfo":{"status":"ok","timestamp":1599135674310,"user_tz":-480,"elapsed":3952024,"user":{"displayName":"lei ya","photoUrl":"","userId":"05028726856435333308"}},"outputId":"74ae9519-8e0f-4750-ce42-d46b139c9d4a"},"source":["df_clean = pd.read_csv(\"./drive/My Drive/total_data.csv\")\n","#---------------------------------------------------------------------------------------\n","given_ingredient = list(df_clean.ingredients)[0:192]\n","#-----------------------------创建评估用词表--------------------------------------------\n","reference_recipe = list(df_clean.recipes)\n","reference_meteor = []\n","references = []\n","reference_list = reference_recipe[0:192]\n","for astr in reference_list:\n","    alist = astr.split(' ')[1:-1]\n","    reference_meteor.append(' '.join(alist))\n","    references.append([alist])\n","clean_summaries = list(df_clean.ingredients)\n","clean_texts = list(df_clean.recipes)\n","sp_dataset = tuple(clean_summaries)\n","en_dataset = tuple(clean_texts)\n","#将词语式数据转成ID式\n","def tokenizer(lang):\n","    lang_tokenizer = keras.preprocessing.text.Tokenizer(\n","        num_words=None, filters='', split=' ')\n","    lang_tokenizer.fit_on_texts(lang)#统计词频，生成词表\n","    tensor = lang_tokenizer.texts_to_sequences(lang)\n","    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding='post')#在句子后面做padding\n","    return tensor, lang_tokenizer\n","\n","\n","input_tensor, input_tokenizer = tokenizer(sp_dataset)\n","output_tensor, output_tokenizer = tokenizer(en_dataset)\n","\n","def max_length(tensor):\n","    return max(len(t) for t in tensor)\n","\n","max_length_input = max_length(input_tensor)\n","max_length_output = max_length(output_tensor)\n","print(max_length_input, max_length_output)\n","\n","from sklearn.model_selection import train_test_split\n","input_train, input_eval, output_train, output_eval = train_test_split(input_tensor[192:], output_tensor[192:], test_size=0.01, shuffle=False)\n","\n","batch_size = 64\n","epochs = 20\n","dataset = tf.data.Dataset.from_tensor_slices((input_train, output_train))\n","train_dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder= True)\n","train_dataset = train_dataset.shuffle(10000)\n","eval_dataset = tf.data.Dataset.from_tensor_slices((input_eval, output_eval))\n","eval_dataset = eval_dataset.repeat(epochs).batch(batch_size, drop_remainder= True)\n","\n","for x, y in train_dataset.take(1):\n","    print(x.shape)\n","    print(y.shape)\n","    print(x)\n","    print(y)\n","\n","embedding_units = 256\n","units = 256\n","#input_tokenizer.word_index是个字典\n","input_vocab_size = len(input_tokenizer.word_index) + 1\n","output_vocab_size = len(output_tokenizer.word_index) + 1\n","print(input_vocab_size,output_vocab_size)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["35 216\n","(64, 35)\n","(64, 216)\n","tf.Tensor(\n","[[  1 612  69 ...   0   0   0]\n"," [  1 106  83 ...   0   0   0]\n"," [  1 106  83 ...   0   0   0]\n"," ...\n"," [  1 519  73 ...   0   0   0]\n"," [  1  14  66 ...   0   0   0]\n"," [  1   7 133 ...   0   0   0]], shape=(64, 35), dtype=int32)\n","tf.Tensor(\n","[[  11   24 1221 ...    0    0    0]\n"," [  11    6    5 ...    0    0    0]\n"," [  11   42   17 ...    0    0    0]\n"," ...\n"," [  11   42   17 ...    0    0    0]\n"," [  11   52   65 ...    0    0    0]\n"," [  11   42   17 ...    0    0    0]], shape=(64, 216), dtype=int32)\n","3492 6728\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fX0xV-4Cq2fP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":100},"executionInfo":{"status":"ok","timestamp":1599135679502,"user_tz":-480,"elapsed":3957213,"user":{"displayName":"lei ya","photoUrl":"","userId":"05028726856435333308"}},"outputId":"4e56cdff-6950-4f6c-925a-349b0a2edf5a"},"source":["class BahdanauAttention(keras.Model):\n","    \n","    def __init__(self, units):\n","        super(BahdanauAttention, self).__init__()\n","        self.W1 = keras.layers.Dense(units)\n","        self.W2 = keras.layers.Dense(units)\n","        self.V = keras.layers.Dense(1)\n","        \n","    def call(self, decoder_hidden, encoder_outputs):\n","        #decoder_hidden.shape = (batch_size, units)\n","        #encoder_outputs.shape = (batch_size, length, units)\n","        decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, 1)#decoder_hidden.shape = (batch_size, 1, units)\n","        #before V:tf.nn.tanh/shape: (batch_size, length, units)\n","        #after V:(batch_size, length, 1)\n","        score = self.V(\n","            tf.nn.tanh(\n","                self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time_axis)))\n","        \n","        #shape: (batch_size, length, 1)\n","        attention_weights = tf.nn.softmax(score, axis = 1)\n","        #context_vector.shape:(batch_size, length, units)\n","        context_vector = attention_weights * encoder_outputs\n","        #context_vector.shape:(batch_size, units)\n","        context_vector = tf.reduce_sum(context_vector, axis = 1)#在length上求和\n","        \n","        return context_vector, attention_weights\n","\n","attention_model = BahdanauAttention(units = 10)#units:经过W1之后的units个数，与batch_size, length, units里units不同\n","attention_results, attention_weights = attention_model.call(sample_hidden,sample_output)\n","\n","print(attention_results.shape)\n","print(attention_weights.shape)\n","\n","#---------------------------------lstm-----------------------------------------\n","class Decoder(keras.Model):\n","    def __init__(self, vocab_size, embedding_units, decoding_units, batch_size):\n","        super(Decoder, self).__init__()#调用父类（keras.Model）的构造函数\n","        self.batch_size = batch_size\n","        self.decoding_units = decoding_units\n","        self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n","        self.gru = keras.layers.LSTM(self.decoding_units,\n","                                   return_sequences = True,\n","                                   return_state = True,\n","                                   recurrent_initializer = 'glorot_uniform')\n","        \n","        self.fc = keras.layers.Dense(vocab_size)\n","        self.attention = BahdanauAttention(self.decoding_units)\n","        \n","        #x:当前步输入，hidden:前一步输出\n","    def call(self, x, hidden, encoding_outputs,encoding_state):\n","        #context_vector.shape: (bathc_size, units)\n","        context_vector, attention_weights = self.attention.call(hidden, encoding_outputs)\n","        #befor embedding:x.shape:(batch_size, 1)\n","        #after embedding:x.shape:(batch_size, 1, embedding_units)\n","        \n","        x = self.embedding(x)\n","        \n","        combined_x = tf.concat([tf.expand_dims(context_vector, 1),x], axis = -1)\n","        #output.shape: (batch_size, 1, decoding_units)\n","        #state.shape: (batch_size, decoding_units)\n","        output, state, state_c = self.gru(combined_x, initial_state = [hidden,encoding_state])\n","        #output = keras.layers.Dropout(0.5)(output)\n","        #output.shape: (batch_size, decoding_units)\n","        output = tf.reshape(output, (-1, output.shape[2]))\n","        \n","        #output.shape: (batch_size, vocab_size)\n","        output = self.fc(output)\n","        \n","        return context_vector, output, state, attention_weights, state_c\n","decoder = Decoder(output_vocab_size, embedding_units, units, batch_size)\n","\n","outputs = decoder.call(tf.random.uniform((batch_size,1)), sample_hidden, sample_output,sample_hidden)\n","\n","context_vector, decoder_output, decoder_hidden, decoder_aw, state_c = outputs\n","\n","print(decoder_output.shape)\n","print(decoder_hidden.shape)\n","print(decoder_aw.shape)\n","\n","optimizer = keras.optimizers.Adam()\n","\n","loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = 'none')\n","#from_logits直接经过fc的输出没有经过softmax,如果经过softmax就设成False\n","\n","def loss_function(real, pred):\n","    #输出里的padding不应该计算到损失函数中去\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))#是padding时，mask取0\n","    loss_ = loss_object(real, pred)\n","    mask = tf.cast(mask, dtype = loss_.dtype)\n","    loss_ *= mask\n","    \n","    return tf.reduce_mean(loss_)\n","\n","W11 = keras.layers.Dense(1)\n","W22 = keras.layers.Dense(1)\n","W33 = keras.layers.Dense(1)\n","\n","\n","#lstm-----------------------------------------\n","#@tf.function#加速cell\n","def train_step(inp, targ, encoding_hidden):\n","    loss = 0\n","    with tf.GradientTape() as tape:\n","        encoding_outputs, encoding_hidden, encoding_c = encoder(inp, encoding_hidden)\n","        inp = inp.numpy()\n","        decoding_hidden = encoding_hidden\n","        decoding_state = encoding_c\n","        #eg: <start> I am here <end>\n","        #1. <start> -> I\n","        #2. I -> am\n","        #3. am -> here\n","        #4. here -> <end>\n","        for t in range(0, targ.shape[1]-1):\n","            decoding_input = tf.expand_dims(targ[:, t],1)\n","            context_vector, predictions, decoding_hidden, decoding_aw, decoding_state = decoder.call(decoding_input, decoding_hidden, encoding_outputs,decoding_state)\n","            #由decoding.call返回的三维decoding_aw\n","            attention_weights = np.reshape(decoding_aw, (-1, 35))\n","            adpre = np.zeros((64,6728))\n","            \n","            for index1,sen in enumerate(inp):\n","                for index2, word_index in enumerate(sen):\n","                #int means only integer can be used as indices\n","                    adpre[index1][int(word_index)] = attention_weights[index1][index2]\n","            #tf.nn.sigmoid之前shape应该是64,1\n","            prob = tf.nn.sigmoid(W11(context_vector) + W22(decoding_hidden) + W33(decoding_input))\n","            #print(prob.shape)\n","            adpre = tf.convert_to_tensor(adpre,tf.float32)\n","            #prob = tf.conver_to_tensor(prob)\n","            #print(prob)\n","            #print(adpre)\n","            new_prediction = (1-prob) * adpre + prob * predictions\n","            loss += loss_function(targ[:, t+1],new_prediction)\n","            \n","    batch_loss = loss / int(targ.shape[0])\n","        \n","    variables = encoder.trainable_variables + decoder.trainable_variables + W11.trainable_variables + W22.trainable_variables + W33.trainable_variables\n","        \n","    gradients = tape.gradient(loss, variables)\n","    optimizer.apply_gradients(zip(gradients, variables))\n","        \n","    return batch_loss   \n","\n","#-------------------------------------------lstm-------------------------\n","def eval_step(inp, targ, encoding_hidden):\n","    loss = 0\n","    \n","    encoding_outputs, encoding_hidden, encoding_c = encoder(inp, encoding_hidden)\n","    inp = inp.numpy()\n","    decoding_hidden = encoding_hidden\n","    decoding_state = encoding_c\n","        #eg: <start> I am here <end>\n","        #1. <start> -> I\n","        #2. I -> am\n","        #3. am -> here\n","        #4. here -> <end>\n","    for t in range(0, targ.shape[1]-1):\n","        decoding_input = tf.expand_dims(targ[:, t],1)\n","        context_vector, predictions, decoding_hidden, decoding_aw, decoding_state = decoder.call(decoding_input, decoding_hidden, encoding_outputs, decoding_state)\n","        attention_weights = np.reshape(decoding_aw, (-1, 35))\n","        adpre = np.zeros((64,6728))\n","        for index1,sen in enumerate(inp):\n","            for index2, word_index in enumerate(sen):\n","                #int means only integer can be used as indices\n","                adpre[index1][int(word_index)] = attention_weights[index1][index2]\n","            #tf.nn.sigmoid之前shape应该是64,1\n","        prob = tf.nn.sigmoid(W11(context_vector) + W22(decoding_hidden) + W33(decoding_input))\n","        #print(prob.shape)\n","        adpre = tf.convert_to_tensor(adpre,tf.float32)\n","        #prob = tf.conver_to_tensor(prob)\n","        new_prediction = (1-prob) * adpre + prob * predictions\n","        loss += loss_function(targ[:, t+1],new_prediction)\n","            \n","    batch_loss = loss / int(targ.shape[0])\n","    return batch_loss "],"execution_count":5,"outputs":[{"output_type":"stream","text":["(64, 256)\n","(64, 18, 1)\n","(64, 6728)\n","(64, 256)\n","(64, 18, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C9EUPm9lq4AC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"1d17b370-2320-4c7a-8c7e-7cb9b1ccbaa8"},"source":["adict = {'training_loss': [], 'validation_loss': []}\n","alist = []\n","import time\n","\n","# epochs = 20\n","epochs = 20\n","# steps_per_epoch = len(input_tensor[192:18973]) // batch_size\n","# steps_per_epoch1 = len(input_tensor[18973:]) // batch_size\n","steps_per_epoch = len(input_train) // batch_size\n","steps_per_epoch1 = len(input_eval) // batch_size\n","for epoch in range(epochs):\n","    start = time.time()\n","\n","    encoding_hidden = encoder.initialize_hidden_state()\n","    total_loss = 0\n","    total_loss1 = 0\n","    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n","        # inp_np = inp.numpy()\n","        # print(type(inp_np))\n","        batch_loss = train_step(inp, targ, encoding_hidden)\n","        total_loss += batch_loss\n","\n","        if batch % 100 == 0:\n","            print('Epoch {} Batch {} Loss {:4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n","    \n","\n","\n","    for (batch1, (inp, targ)) in enumerate(eval_dataset.take(steps_per_epoch1)):\n","        eval_loss = eval_step(inp, targ, encoding_hidden)\n","        total_loss1 += eval_loss\n","\n","    print('Epoch {} Loss {:4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n","    print('Epoch {} Eval_Loss {:4f}'.format(epoch + 1, total_loss1 / steps_per_epoch1))\n","    print('Time take for 1 epoch {} sec\\n'.format(time.time() - start))\n","    adict['training_loss'].append(round(float(total_loss / steps_per_epoch), 3))\n","    adict['validation_loss'].append(round(float(total_loss1 / steps_per_epoch1), 3))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1 Batch 0 Loss 12.083158\n","Epoch 1 Batch 100 Loss 7.802237\n","Epoch 1 Batch 200 Loss 7.161674\n","Epoch 1 Loss 7.713625\n","Epoch 1 Eval_Loss 6.615302\n","Time take for 1 epoch 1780.0943756103516 sec\n","\n","Epoch 2 Batch 0 Loss 5.432662\n","Epoch 2 Batch 100 Loss 5.018457\n","Epoch 2 Batch 200 Loss 4.950075\n","Epoch 2 Loss 5.293014\n","Epoch 2 Eval_Loss 5.179445\n","Time take for 1 epoch 1768.9061002731323 sec\n","\n","Epoch 3 Batch 0 Loss 4.723670\n","Epoch 3 Batch 100 Loss 4.118921\n","Epoch 3 Batch 200 Loss 3.933419\n","Epoch 3 Loss 4.273176\n","Epoch 3 Eval_Loss 4.494293\n","Time take for 1 epoch 1785.700649023056 sec\n","\n","Epoch 4 Batch 0 Loss 4.086216\n","Epoch 4 Batch 100 Loss 3.849802\n","Epoch 4 Batch 200 Loss 3.366705\n","Epoch 4 Loss 3.825267\n","Epoch 4 Eval_Loss 4.207370\n","Time take for 1 epoch 1774.8981857299805 sec\n","\n","Epoch 5 Batch 0 Loss 3.367610\n","Epoch 5 Batch 100 Loss 3.521459\n","Epoch 5 Batch 200 Loss 3.675471\n","Epoch 5 Loss 3.534593\n","Epoch 5 Eval_Loss 4.025576\n","Time take for 1 epoch 1783.8643674850464 sec\n","\n","Epoch 6 Batch 0 Loss 3.544342\n","Epoch 6 Batch 100 Loss 3.928494\n","Epoch 6 Batch 200 Loss 3.345819\n","Epoch 6 Loss 3.366202\n","Epoch 6 Eval_Loss 3.871655\n","Time take for 1 epoch 1785.180568933487 sec\n","\n","Epoch 7 Batch 0 Loss 2.907357\n","Epoch 7 Batch 100 Loss 3.193786\n","Epoch 7 Batch 200 Loss 3.661125\n","Epoch 7 Loss 3.225810\n","Epoch 7 Eval_Loss 3.763622\n","Time take for 1 epoch 1775.4617404937744 sec\n","\n","Epoch 8 Batch 0 Loss 3.248417\n","Epoch 8 Batch 100 Loss 2.869000\n","Epoch 8 Batch 200 Loss 3.510370\n","Epoch 8 Loss 3.115090\n","Epoch 8 Eval_Loss 3.698611\n","Time take for 1 epoch 1753.8229749202728 sec\n","\n","Epoch 9 Batch 0 Loss 2.688720\n","Epoch 9 Batch 100 Loss 2.787839\n","Epoch 9 Batch 200 Loss 3.374388\n","Epoch 9 Loss 2.984015\n","Epoch 9 Eval_Loss 3.616851\n","Time take for 1 epoch 1744.6250188350677 sec\n","\n","Epoch 10 Batch 0 Loss 3.032734\n","Epoch 10 Batch 100 Loss 2.195856\n","Epoch 10 Batch 200 Loss 2.843174\n","Epoch 10 Loss 2.908293\n","Epoch 10 Eval_Loss 3.571147\n","Time take for 1 epoch 1736.263065814972 sec\n","\n","Epoch 11 Batch 0 Loss 2.893082\n","Epoch 11 Batch 100 Loss 3.297390\n","Epoch 11 Batch 200 Loss 2.342701\n","Epoch 11 Loss 2.808628\n","Epoch 11 Eval_Loss 3.529671\n","Time take for 1 epoch 1746.6820826530457 sec\n","\n","Epoch 12 Batch 0 Loss 2.943885\n","Epoch 12 Batch 100 Loss 2.790330\n","Epoch 12 Batch 200 Loss 2.609781\n","Epoch 12 Loss 2.735662\n","Epoch 12 Eval_Loss 3.479148\n","Time take for 1 epoch 1740.1560053825378 sec\n","\n","Epoch 13 Batch 0 Loss 2.472021\n","Epoch 13 Batch 100 Loss 2.844114\n","Epoch 13 Batch 200 Loss 2.653370\n","Epoch 13 Loss 2.699860\n","Epoch 13 Eval_Loss 3.441814\n","Time take for 1 epoch 1742.0951902866364 sec\n","\n","Epoch 14 Batch 0 Loss 2.416132\n","Epoch 14 Batch 100 Loss 2.891705\n","Epoch 14 Batch 200 Loss 2.722953\n","Epoch 14 Loss 2.623391\n","Epoch 14 Eval_Loss 3.428244\n","Time take for 1 epoch 1733.5404632091522 sec\n","\n","Epoch 15 Batch 0 Loss 1.564265\n","Epoch 15 Batch 100 Loss 2.564220\n","Epoch 15 Batch 200 Loss 2.244849\n","Epoch 15 Loss 2.533046\n","Epoch 15 Eval_Loss 3.410379\n","Time take for 1 epoch 1737.3807475566864 sec\n","\n","Epoch 16 Batch 0 Loss 2.734276\n","Epoch 16 Batch 100 Loss 2.487867\n","Epoch 16 Batch 200 Loss 2.775084\n","Epoch 16 Loss 2.483389\n","Epoch 16 Eval_Loss 3.402610\n","Time take for 1 epoch 1732.6035377979279 sec\n","\n","Epoch 17 Batch 0 Loss 2.339250\n","Epoch 17 Batch 100 Loss 2.519424\n","Epoch 17 Batch 200 Loss 2.354334\n","Epoch 17 Loss 2.434241\n","Epoch 17 Eval_Loss 3.386023\n","Time take for 1 epoch 1730.3901915550232 sec\n","\n","Epoch 18 Batch 0 Loss 2.705549\n","Epoch 18 Batch 100 Loss 2.650817\n","Epoch 18 Batch 200 Loss 2.444523\n","Epoch 18 Loss 2.384328\n","Epoch 18 Eval_Loss 3.402482\n","Time take for 1 epoch 1724.3521916866302 sec\n","\n","Epoch 19 Batch 0 Loss 1.871496\n","Epoch 19 Batch 100 Loss 2.979585\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xpBZ-GbKq589","colab_type":"code","colab":{}},"source":["print(adict) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-28YCCiRq7Xr","colab_type":"code","colab":{}},"source":["#Caution: 在不画图的时候一定要将下面两行代码注释掉\n","import matplotlib.ticker as ticker\n","#-------------------lstm----------------\n","def evaluate(input_sentence):\n","    attention_matrix = np.zeros((max_length_output, max_length_input))\n","    \n","    \n","    inputs = [input_tokenizer.word_index[token] for token in input_sentence.split(' ')]\n","    inputs = keras.preprocessing.sequence.pad_sequences(\n","    [inputs], maxlen=max_length_input, padding='post')\n","    \n","    inputs = tf.convert_to_tensor(inputs)\n","    results = ''\n","    #encoding_hidden = encoder.initialize_hidden_state()\n","    #这里的encoding_hidden在lstm情况下需要改一下维度\n","    encoding_hidden = [tf.zeros((1,units)),tf.zeros((1,units))]\n","    \n","    encoding_outputs, encoding_hidden, encoding_c = encoder(inputs, encoding_hidden)\n","    decoding_hidden = encoding_hidden\n","    decoding_state = encoding_c\n","    #eg: <start> I am here <end>\n","        #1. <start> -> I\n","        #2. I -> am\n","        #3. am -> here\n","        #4. here -> <end>\n","    #decoding_inpu.shape :(1,1)(batch_size, length)\n","    decoding_input = tf.expand_dims([output_tokenizer.word_index['<start>']],0)\n","    for t in range(max_length_output):\n","        #attention_weights.shape: (batch_size, input_length, 1)(1,16,1)\n","        context_vector, predictions, decoding_hidden, decoding_aw, decoding_state = decoder(decoding_input, decoding_hidden, encoding_outputs, decoding_state)\n","        \n","         #-------------------------------------------------------------------------\n","        attention_weights = np.reshape(decoding_aw, (-1, 35))\n","        #经历过上一步后attention_weights.shape = 1,35\n","        adpre = np.zeros((1,6728))\n","        for index1,sen in enumerate(inputs):\n","            for index2, word_index in enumerate(sen):\n","                #int means only integer can be used as indices\n","                adpre[index1][int(word_index)] = attention_weights[index1][index2]\n","            #tf.nn.sigmoid之前shape应该是64,1\n","        prob = tf.nn.sigmoid(W11(context_vector) + W22(decoding_hidden) + W33(decoding_input))\n","        #print(prob.shape)\n","        adpre = tf.convert_to_tensor(adpre,tf.float32)\n","        #prob = tf.conver_to_tensor(prob)\n","        new_prediction = (1-prob) * adpre + prob * predictions\n","        #-------------------------------------------------------------------------------\n","        \n","        attention_weights = tf.reshape(decoding_aw, (-1,))#length=16的向量\n","        attention_matrix[t] = attention_weights.numpy()#attention_weights是一个tensor，用numpy（）取出它的值\n","        #predictions.shape: (batch_size, vocab_size)  (1,4935)\n","        \n","        predicted_id = tf.argmax(new_prediction[0]).numpy()\n","        \n","        results += output_tokenizer.index_word[predicted_id] + ' '\n","        if output_tokenizer.index_word[predicted_id] == '<end>':\n","            return results, input_sentence, attention_matrix\n","        \n","        decoding_input = tf.expand_dims([predicted_id],0)\n","    return results, input_sentence, attention_matrix\n","\n","def plot_attention(attention_matrix, input_sentence, predicted_sentence):\n","    fig = plt.figure(figsize=(10,10))\n","    ax = fig.add_subplot(1,1,1)#add a subplot,1,1,1表示子图位置\n","    ax.matshow(attention_matrix, cmap='viridis')#viridis是一种配色方案\n","    \n","    font_dict = {'fontsize': 14}\n","    ax.set_xticklabels([''] + input_sentence, fontdict=font_dict, rotation = 90)#seq2seq里不需要加空格\n","    ax.set_yticklabels([''] + predicted_sentence, fontdict=font_dict)\n","    plt.show()\n","\n","def translate(input_sentence):\n","    results, input_sentence, attention_matrix = evaluate(input_sentence)\n","    \n","    #------------------------\n","    \n","    print('Input: %s' % (input_sentence))\n","    print('Predicted translation: %s' % (results))\n","    \n","    #attention_matrix = attention_matrix[:len(results.split(' ')), :len(input_sentence.split(' '))]\n","    \n","    #plot_attention(attention_matrix, input_sentence.split(' '), results.split(' '))\n","    return results\n","\n","\n","#----------------------BLEU and Meteor------------------------------\n","candidates = []\n","candidates_meteor = []\n","for sample1 in clean_summaries[0:192]:\n","    #res = translate(sample1+', ')\n","    res = translate(sample1)\n","    candidates_meteor.append(res)\n","    candidates.append(res.split(' '))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eGcuZAGeq80_","colab_type":"code","colab":{}},"source":["from nltk.translate.bleu_score import corpus_bleu\n","score = corpus_bleu(references, candidates,weights=(1,0,0,0))\n","score1 = corpus_bleu(references, candidates,weights=(0.33,0.33,0.33,0))\n","score2 = corpus_bleu(references, candidates,weights=(0.25,0.25,0.25,0.25))\n","print(score,score1,score2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rlb8pnD8q-f_","colab_type":"code","colab":{}},"source":["def calculate_given_item(given_ingredient, generated_recipe):\n","    '''\n","    given_ingredient = ['egg tomatoes','chicken']\n","    splitted_ingredient = [['egg','tomatoes'],['chicken']]\n","    generated_recipe = [['use','egg'],['use','chicken'],...]\n","    '''\n","\n","    total_score = 0\n","    extra_score = 0\n","    splitted_ingredient = []\n","    for bstr in given_ingredient:\n","        splitted_ingredient.append(bstr.split(' '))\n","\n","    for index, each_sample in enumerate(splitted_ingredient):\n","        # count1 covered items\n","        # count2 extra items\n","        count1 = 0\n","        count2 = 0\n","        for each_word in each_sample:\n","            # generated_recipe就是下面的candidates=[['use','egg'],['use','chicken'],...]\n","            if each_word in generated_recipe[index]:\n","                count1 += 1\n","            else:\n","                count2 += 1\n","        extra_score += count2 / len(each_sample)\n","        total_score += count1 / len(each_sample)\n","    # -------------------------------------------------------------------------\n","    for index, each_recipe in enumerate(generated_recipe):\n","        # count1 covered items\n","        # count2 extra items\n","        extra_item = 0\n","        for each_word in each_recipe:\n","            # generated_recipe就是下面的candidates=[['use','egg'],['use','chicken'],...]\n","            if each_word in input_tokenizer.word_index and each_word not in splitted_ingredient[index]:\n","                extra_item += 1\n","\n","    # --------------------------------------------------------------------------\n","\n","    total_score = total_score / len(given_ingredient)\n","\n","    return total_score, extra_item\n","\n","cover_item, extra_item = calculate_given_item(given_ingredient, candidates)\n","print(cover_item,extra_item)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bjt8upAsq_xV","colab_type":"code","colab":{}},"source":["print(reference_meteor)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1og3Ft2DrBKO","colab_type":"code","colab":{}},"source":["print(candidates_meteor)"],"execution_count":null,"outputs":[]}]}