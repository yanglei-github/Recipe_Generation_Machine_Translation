{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"copynet_withvae_lstm_decoder.ipynb","provenance":[],"mount_file_id":"1SpsvcUy_PlwKUi7B0XrQGQO4hvdHXOF5","authorship_tag":"ABX9TyOepSmNjAaJRt5OVundhM5O"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"AtCstnWdDe_1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":485},"executionInfo":{"status":"ok","timestamp":1598352590618,"user_tz":-480,"elapsed":9377,"user":{"displayName":"lei ya","photoUrl":"","userId":"05028726856435333308"}},"outputId":"1801b07e-551a-4e0b-b2da-2a50764aae34"},"source":["from nltk.corpus import stopwords\n","import tensorflow as tf\n","import matplotlib as mpl\n","import matplotlib.pyplot as  plt\n","import numpy as np\n","import pandas as pd\n","import sklearn\n","import os\n","import sys\n","import time\n","from tensorflow import keras\n","\n","import tensorflow as tf\n","#fitting for gpu\n","\n","\n","config = tf.compat.v1.ConfigProto()\n","config.gpu_options.allow_growth = True\n","session = tf.compat.v1.Session(config=config)\n","\n","df_clean = pd.read_csv(\"./drive/My Drive/total_data.csv\")\n","#---------------------------------------------------------------------------------------\n","given_ingredient = list(df_clean.ingredients)[0:192]\n","#-----------------------------创建评估用词表--------------------------------------------\n","reference_recipe = list(df_clean.recipes)\n","reference_meteor = []\n","references = []\n","reference_list = reference_recipe[0:192]\n","for astr in reference_list:\n","    alist = astr.split(' ')[1:-1]\n","    reference_meteor.append(' '.join(alist))\n","    references.append([alist])\n","clean_summaries = list(df_clean.ingredients)\n","clean_texts = list(df_clean.recipes)\n","sp_dataset = tuple(clean_summaries)\n","en_dataset = tuple(clean_texts)\n","#将词语式数据转成ID式\n","def tokenizer(lang):\n","    lang_tokenizer = keras.preprocessing.text.Tokenizer(\n","        num_words=None, filters='', split=' ')\n","    lang_tokenizer.fit_on_texts(lang)#统计词频，生成词表\n","    tensor = lang_tokenizer.texts_to_sequences(lang)\n","    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding='post')#在句子后面做padding\n","    return tensor, lang_tokenizer\n","\n","input_tensor, input_tokenizer = tokenizer(sp_dataset)\n","output_tensor, output_tokenizer = tokenizer(en_dataset)\n","\n","def max_length(tensor):\n","    return max(len(t) for t in tensor)\n","\n","max_length_input = max_length(input_tensor)\n","max_length_output = max_length(output_tensor)\n","print(max_length_input, max_length_output)\n","\n","from sklearn.model_selection import train_test_split\n","input_train, input_eval, output_train, output_eval = train_test_split(input_tensor[192:], output_tensor[192:], test_size=0.01, shuffle=False)\n","\n","batch_size = 64\n","epochs = 20\n","dataset = tf.data.Dataset.from_tensor_slices((input_train, output_train))\n","train_dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder= True)\n","train_dataset = train_dataset.shuffle(10000)\n","eval_dataset = tf.data.Dataset.from_tensor_slices((input_eval, output_eval))\n","eval_dataset = eval_dataset.repeat(epochs).batch(batch_size, drop_remainder= True)\n","\n","for x, y in train_dataset.take(1):\n","    print(x.shape)\n","    print(y.shape)\n","    print(x)\n","    print(y)\n","\n","embedding_units = 256\n","units = 256\n","#input_tokenizer.word_index是个字典\n","input_vocab_size = len(input_tokenizer.word_index) + 1\n","output_vocab_size = len(output_tokenizer.word_index) + 1\n","print(input_vocab_size,output_vocab_size)\n","\n","#-------------------lstm-----------------------------------\n","class Encoder(keras.Model):\n","    def __init__(self, vocab_size, embedding_units, encoding_units, batch_size):\n","        super(Encoder, self).__init__()\n","        self.batch_size = batch_size\n","        self.encoding_units = encoding_units\n","        self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n","        #由于用attention，每步输出需要return_sequences = True\n","        self.gru = keras.layers.LSTM(self.encoding_units,\n","                                   return_sequences = True,\n","                                   return_state = True,\n","                                   recurrent_initializer = 'glorot_uniform')\n","        \n","        \n","    #hidden是初始化的隐含状态\n","    #这里hidden应该变成[hiddenstate,hiddenc]\n","    def call(self, x, hidden):\n","        x = self.embedding(x)\n","        output, state, state_c = self.gru(x, initial_state = hidden)\n","        #output = keras.layers.Dropout(0.75)(output)\n","        return output, state, state_c\n","    \n","    def initialize_hidden_state(self):\n","        return [tf.zeros((self.batch_size, self.encoding_units)),tf.zeros((self.batch_size, self.encoding_units))]\n","    \n","encoder = Encoder(input_vocab_size, embedding_units, units, batch_size)\n","sample_hidden = encoder.initialize_hidden_state()\n","sample_output, sample_hidden, sample_c = encoder.call(x, sample_hidden)\n","\n","print('sample_output.shape: ', sample_output.shape)\n","print('sample_hidden.shape: ', sample_hidden.shape)\n","print('sample_memory.shape: ', sample_c.shape)\n","\n","class BahdanauAttention(keras.Model):\n","    \n","    def __init__(self, units):\n","        super(BahdanauAttention, self).__init__()\n","        self.W1 = keras.layers.Dense(units)\n","        self.W2 = keras.layers.Dense(units)\n","        self.V = keras.layers.Dense(1)\n","        \n","    def call(self, decoder_hidden, encoder_outputs):\n","        #decoder_hidden.shape = (batch_size, units)\n","        #encoder_outputs.shape = (batch_size, length, units)\n","        decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, 1)#decoder_hidden.shape = (batch_size, 1, units)\n","        #before V:tf.nn.tanh/shape: (batch_size, length, units)\n","        #after V:(batch_size, length, 1)\n","        score = self.V(\n","            tf.nn.tanh(\n","                self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time_axis)))\n","        \n","        #shape: (batch_size, length, 1)\n","        attention_weights = tf.nn.softmax(score, axis = 1)\n","        #context_vector.shape:(batch_size, length, units)\n","        context_vector = attention_weights * encoder_outputs\n","        #context_vector.shape:(batch_size, units)\n","        context_vector = tf.reduce_sum(context_vector, axis = 1)#在length上求和\n","        \n","        return context_vector, attention_weights\n","\n","attention_model = BahdanauAttention(units = 10)#units:经过W1之后的units个数，与batch_size, length, units里units不同\n","attention_results, attention_weights = attention_model.call(sample_hidden,sample_output)\n","\n","print(attention_results.shape)\n","print(attention_weights.shape)\n","\n","#---------------------------------lstm-----------------------------------------\n","class Decoder(keras.Model):\n","    def __init__(self, vocab_size, embedding_units, decoding_units, batch_size):\n","        super(Decoder, self).__init__()#调用父类（keras.Model）的构造函数\n","        self.batch_size = batch_size\n","        self.decoding_units = decoding_units\n","        self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n","        self.gru = keras.layers.LSTM(self.decoding_units,\n","                                   return_sequences = True,\n","                                   return_state = True,\n","                                   recurrent_initializer = 'glorot_uniform')\n","        \n","        self.fc = keras.layers.Dense(vocab_size)\n","        self.attention = BahdanauAttention(self.decoding_units)\n","        \n","        #x:当前步输入，hidden:前一步输出\n","    def call(self, x, hidden, encoding_outputs,encoding_state):\n","        #context_vector.shape: (bathc_size, units)\n","        context_vector, attention_weights = self.attention.call(hidden, encoding_outputs)\n","        #befor embedding:x.shape:(batch_size, 1)\n","        #after embedding:x.shape:(batch_size, 1, embedding_units)\n","        \n","        x = self.embedding(x)\n","        \n","        combined_x = tf.concat([tf.expand_dims(context_vector, 1),x], axis = -1)\n","        #output.shape: (batch_size, 1, decoding_units)\n","        #state.shape: (batch_size, decoding_units)\n","        output, state, state_c = self.gru(combined_x, initial_state = [hidden,encoding_state])\n","        #output = keras.layers.Dropout(0.5)(output)\n","        #output.shape: (batch_size, decoding_units)\n","        output = tf.reshape(output, (-1, output.shape[2]))\n","        \n","        #output.shape: (batch_size, vocab_size)\n","        output = self.fc(output)\n","        \n","        return context_vector, output, state, attention_weights, state_c\n","decoder = Decoder(output_vocab_size, embedding_units, units, batch_size)\n","\n","outputs = decoder.call(tf.random.uniform((batch_size,1)), sample_hidden, sample_output,sample_hidden)\n","\n","context_vector, decoder_output, decoder_hidden, decoder_aw, state_c = outputs\n","\n","print(decoder_output.shape)\n","print(decoder_hidden.shape)\n","print(decoder_aw.shape)\n","\n","optimizer = keras.optimizers.Adam()\n","\n","loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = 'none')\n","#from_logits直接经过fc的输出没有经过softmax,如果经过softmax就设成False\n","\n","def loss_function(real, pred):\n","    #输出里的padding不应该计算到损失函数中去\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))#是padding时，mask取0\n","    loss_ = loss_object(real, pred)\n","    mask = tf.cast(mask, dtype = loss_.dtype)\n","    loss_ *= mask\n","    \n","    return tf.reduce_mean(loss_)\n","\n","W11 = keras.layers.Dense(1)\n","W22 = keras.layers.Dense(1)\n","W33 = keras.layers.Dense(1)\n","\n","\n","#lstm-----------------------------------------\n","#@tf.function#加速cell\n","def train_step(inp, targ, encoding_hidden):\n","    loss = 0\n","    with tf.GradientTape() as tape:\n","        encoding_outputs, encoding_hidden, encoding_c = encoder(inp, encoding_hidden)\n","        inp = inp.numpy()\n","        decoding_hidden = encoding_hidden\n","        decoding_state = encoding_c\n","        #eg: <start> I am here <end>\n","        #1. <start> -> I\n","        #2. I -> am\n","        #3. am -> here\n","        #4. here -> <end>\n","        for t in range(0, targ.shape[1]-1):\n","            decoding_input = tf.expand_dims(targ[:, t],1)\n","            context_vector, predictions, decoding_hidden, decoding_aw, decoding_state = decoder.call(decoding_input, decoding_hidden, encoding_outputs,decoding_state)\n","            #由decoding.call返回的三维decoding_aw\n","            attention_weights = np.reshape(decoding_aw, (-1, 35))\n","            adpre = np.zeros((64,6728))\n","            \n","            for index1,sen in enumerate(inp):\n","                for index2, word_index in enumerate(sen):\n","                #int means only integer can be used as indices\n","                    adpre[index1][int(word_index)] = attention_weights[index1][index2]\n","            #tf.nn.sigmoid之前shape应该是64,1\n","            prob = tf.nn.sigmoid(W11(context_vector) + W22(decoding_hidden) + W33(decoding_input))\n","            #print(prob.shape)\n","            adpre = tf.convert_to_tensor(adpre,tf.float32)\n","            #prob = tf.conver_to_tensor(prob)\n","            #print(prob)\n","            #print(adpre)\n","            new_prediction = (1-prob) * adpre + prob * predictions\n","            loss += loss_function(targ[:, t+1],new_prediction)\n","            \n","    batch_loss = loss / int(targ.shape[0])\n","        \n","    variables = encoder.trainable_variables + decoder.trainable_variables + W11.trainable_variables + W22.trainable_variables + W33.trainable_variables\n","        \n","    gradients = tape.gradient(loss, variables)\n","    optimizer.apply_gradients(zip(gradients, variables))\n","        \n","    return batch_loss   \n","\n","#-------------------------------------------lstm-------------------------\n","def eval_step(inp, targ, encoding_hidden):\n","    loss = 0\n","    \n","    encoding_outputs, encoding_hidden, encoding_c = encoder(inp, encoding_hidden)\n","    inp = inp.numpy()\n","    decoding_hidden = encoding_hidden\n","    decoding_state = encoding_c\n","        #eg: <start> I am here <end>\n","        #1. <start> -> I\n","        #2. I -> am\n","        #3. am -> here\n","        #4. here -> <end>\n","    for t in range(0, targ.shape[1]-1):\n","        decoding_input = tf.expand_dims(targ[:, t],1)\n","        context_vector, predictions, decoding_hidden, decoding_aw, decoding_state = decoder.call(decoding_input, decoding_hidden, encoding_outputs, decoding_state)\n","        attention_weights = np.reshape(decoding_aw, (-1, 35))\n","        adpre = np.zeros((64,6728))\n","        for index1,sen in enumerate(inp):\n","            for index2, word_index in enumerate(sen):\n","                #int means only integer can be used as indices\n","                adpre[index1][int(word_index)] = attention_weights[index1][index2]\n","            #tf.nn.sigmoid之前shape应该是64,1\n","        prob = tf.nn.sigmoid(W11(context_vector) + W22(decoding_hidden) + W33(decoding_input))\n","        #print(prob.shape)\n","        adpre = tf.convert_to_tensor(adpre,tf.float32)\n","        #prob = tf.conver_to_tensor(prob)\n","        new_prediction = (1-prob) * adpre + prob * predictions\n","        loss += loss_function(targ[:, t+1],new_prediction)\n","            \n","    batch_loss = loss / int(targ.shape[0])\n","    return batch_loss "],"execution_count":1,"outputs":[{"output_type":"stream","text":["35 216\n","(64, 35)\n","(64, 216)\n","tf.Tensor(\n","[[  1 130 107 ...   0   0   0]\n"," [  1  16  47 ...   0   0   0]\n"," [  1  27   8 ...   0   0   0]\n"," ...\n"," [  1  25  74 ...   0   0   0]\n"," [  1   6   3 ...   0   0   0]\n"," [  1  93  38 ...   0   0   0]], shape=(64, 35), dtype=int32)\n","tf.Tensor(\n","[[  11   52 2369 ...    0    0    0]\n"," [  11  517  271 ...    0    0    0]\n"," [  11    6    5 ...    0    0    0]\n"," ...\n"," [  11   42   17 ...    0    0    0]\n"," [  11   50   29 ...    0    0    0]\n"," [  11   42   17 ...    0    0    0]], shape=(64, 216), dtype=int32)\n","3492 6728\n","sample_output.shape:  (64, 35, 256)\n","sample_hidden.shape:  (64, 256)\n","sample_memory.shape:  (64, 256)\n","(64, 256)\n","(64, 35, 1)\n","(64, 6728)\n","(64, 256)\n","(64, 35, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"733SCm5IDk0u","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1598335649996,"user_tz":-480,"elapsed":2012439,"user":{"displayName":"lei ya","photoUrl":"","userId":"05028726856435333308"}},"outputId":"92cf2343-d929-43e0-dec6-e1350f2c3310"},"source":["#pretraining for 10 epochs\n","adict = {'training_loss':[], 'validation_loss':[]}\n","alist = []\n","import time\n","#epochs = 20\n","epochs = 10\n","# steps_per_epoch = len(input_tensor[192:18973]) // batch_size\n","# steps_per_epoch1 = len(input_tensor[18973:]) // batch_size\n","steps_per_epoch = len(input_train) // batch_size\n","steps_per_epoch1 = len(input_eval) // batch_size\n","for epoch in range(epochs):\n","    start = time.time()\n","    \n","    encoding_hidden = encoder.initialize_hidden_state()\n","    total_loss = 0\n","    total_loss1 = 0\n","    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n","        \n","        batch_loss = train_step(inp, inp, encoding_hidden)\n","        total_loss += batch_loss\n","        \n","        if batch % 100 == 0:\n","            print('Epoch {} Batch {} Loss {:4f}'.format(epoch+1, batch, batch_loss.numpy()))\n","    for (batch1, (inp, targ)) in enumerate(eval_dataset.take(steps_per_epoch1)):\n","        eval_loss = eval_step(inp, inp, encoding_hidden)\n","        total_loss1 += eval_loss\n","               \n","            \n","      \n","    print('Epoch {} Loss {:4f}'.format(epoch+1, total_loss / steps_per_epoch))\n","    print('Epoch {} Eval_Loss {:4f}'.format(epoch+1, total_loss1 / steps_per_epoch1))\n","    print('Time take for 1 epoch {} sec\\n'.format(time.time() - start))\n","    adict['training_loss'].append(round(float(total_loss / steps_per_epoch),3))\n","    adict['validation_loss'].append(round(float(total_loss1 / steps_per_epoch1),3))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Epoch 1 Batch 0 Loss 2.235138\n","Epoch 1 Batch 100 Loss 1.893021\n","Epoch 1 Batch 200 Loss 1.473694\n","Epoch 1 Loss 1.845392\n","Epoch 1 Eval_Loss 1.856920\n","Time take for 1 epoch 199.52124667167664 sec\n","\n","Epoch 2 Batch 0 Loss 1.818720\n","Epoch 2 Batch 100 Loss 1.355804\n","Epoch 2 Batch 200 Loss 1.599576\n","Epoch 2 Loss 1.436089\n","Epoch 2 Eval_Loss 1.565011\n","Time take for 1 epoch 201.25531840324402 sec\n","\n","Epoch 3 Batch 0 Loss 1.142384\n","Epoch 3 Batch 100 Loss 1.303381\n","Epoch 3 Batch 200 Loss 1.450739\n","Epoch 3 Loss 1.158600\n","Epoch 3 Eval_Loss 1.387381\n","Time take for 1 epoch 201.03059267997742 sec\n","\n","Epoch 4 Batch 0 Loss 0.854429\n","Epoch 4 Batch 100 Loss 0.883327\n","Epoch 4 Batch 200 Loss 0.921095\n","Epoch 4 Loss 0.997736\n","Epoch 4 Eval_Loss 1.293783\n","Time take for 1 epoch 200.0273585319519 sec\n","\n","Epoch 5 Batch 0 Loss 0.842997\n","Epoch 5 Batch 100 Loss 0.950765\n","Epoch 5 Batch 200 Loss 0.812208\n","Epoch 5 Loss 0.909055\n","Epoch 5 Eval_Loss 1.202146\n","Time take for 1 epoch 198.977064371109 sec\n","\n","Epoch 6 Batch 0 Loss 0.836204\n","Epoch 6 Batch 100 Loss 0.733389\n","Epoch 6 Batch 200 Loss 0.769589\n","Epoch 6 Loss 0.849755\n","Epoch 6 Eval_Loss 1.168278\n","Time take for 1 epoch 200.15549850463867 sec\n","\n","Epoch 7 Batch 0 Loss 0.994007\n","Epoch 7 Batch 100 Loss 0.730583\n","Epoch 7 Batch 200 Loss 0.734977\n","Epoch 7 Loss 0.813148\n","Epoch 7 Eval_Loss 1.124995\n","Time take for 1 epoch 199.46296453475952 sec\n","\n","Epoch 8 Batch 0 Loss 0.879034\n","Epoch 8 Batch 100 Loss 0.798054\n","Epoch 8 Batch 200 Loss 0.714120\n","Epoch 8 Loss 0.769562\n","Epoch 8 Eval_Loss 1.098042\n","Time take for 1 epoch 200.49501514434814 sec\n","\n","Epoch 9 Batch 0 Loss 0.705918\n","Epoch 9 Batch 100 Loss 0.671159\n","Epoch 9 Batch 200 Loss 0.671273\n","Epoch 9 Loss 0.744807\n","Epoch 9 Eval_Loss 1.060722\n","Time take for 1 epoch 201.47469305992126 sec\n","\n","Epoch 10 Batch 0 Loss 0.722971\n","Epoch 10 Batch 100 Loss 0.737851\n","Epoch 10 Batch 200 Loss 0.716778\n","Epoch 10 Loss 0.716717\n","Epoch 10 Eval_Loss 1.044127\n","Time take for 1 epoch 201.4162437915802 sec\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KLhNDicxDoBv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1598335649999,"user_tz":-480,"elapsed":2012436,"user":{"displayName":"lei ya","photoUrl":"","userId":"05028726856435333308"}},"outputId":"bf5fbcfe-4139-483b-eb65-b290de5432b3"},"source":["print(adict)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["{'training_loss': [1.845, 1.436, 1.159, 0.998, 0.909, 0.85, 0.813, 0.77, 0.745, 0.717], 'validation_loss': [1.857, 1.565, 1.387, 1.294, 1.202, 1.168, 1.125, 1.098, 1.061, 1.044]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Mv_jrtyQDqVm","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598352602316,"user_tz":-480,"elapsed":1358,"user":{"displayName":"lei ya","photoUrl":"","userId":"05028726856435333308"}}},"source":["#-------------------lstm-----------------------------------\n","class Encoder(keras.Model):\n","    def __init__(self, vocab_size, embedding_units, encoding_units, batch_size):\n","        super(Encoder, self).__init__()\n","        self.batch_size = batch_size\n","        self.encoding_units = encoding_units\n","        self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n","        #由于用attention，每步输出需要return_sequences = True\n","        self.gru = keras.layers.LSTM(self.encoding_units,\n","                                   return_sequences = True,\n","                                   return_state = True,\n","                                   recurrent_initializer = 'glorot_uniform')\n","        \n","        \n","    #hidden是初始化的隐含状态\n","    #这里hidden应该变成[hiddenstate,hiddenc]\n","    def call(self, x, hidden):\n","        x = self.embedding(x)\n","        output, state, state_c = self.gru(x, initial_state = hidden)\n","        #output = keras.layers.Dropout(0.75)(output)\n","        return output, state, state_c\n","    \n","    def initialize_hidden_state(self):\n","        return [tf.zeros((self.batch_size, self.encoding_units)),tf.zeros((self.batch_size, self.encoding_units))]\n","    \n","encoder = Encoder(input_vocab_size, embedding_units, units, batch_size)\n","sample_hidden = encoder.initialize_hidden_state()\n","sample_output, sample_hidden, sample_c = encoder.call(x, sample_hidden)\n","\n","optimizer = keras.optimizers.Adam()\n","\n","loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = 'none')\n","#from_logits直接经过fc的输出没有经过softmax,如果经过softmax就设成False\n","\n","def loss_function(real, pred):\n","    #输出里的padding不应该计算到损失函数中去\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))#是padding时，mask取0\n","    loss_ = loss_object(real, pred)\n","    mask = tf.cast(mask, dtype = loss_.dtype)\n","    loss_ *= mask\n","    \n","    return tf.reduce_mean(loss_)\n","\n","W11 = keras.layers.Dense(1)\n","W22 = keras.layers.Dense(1)\n","W33 = keras.layers.Dense(1)\n","\n","\n","#lstm-----------------------------------------\n","#@tf.function#加速cell\n","def train_step(inp, targ, encoding_hidden):\n","    loss = 0\n","    with tf.GradientTape() as tape:\n","        encoding_outputs, encoding_hidden, encoding_c = encoder(inp, encoding_hidden)\n","        inp = inp.numpy()\n","        decoding_hidden = encoding_hidden\n","        decoding_state = encoding_c\n","        #eg: <start> I am here <end>\n","        #1. <start> -> I\n","        #2. I -> am\n","        #3. am -> here\n","        #4. here -> <end>\n","        for t in range(0, targ.shape[1]-1):\n","            decoding_input = tf.expand_dims(targ[:, t],1)\n","            context_vector, predictions, decoding_hidden, decoding_aw, decoding_state = decoder.call(decoding_input, decoding_hidden, encoding_outputs,decoding_state)\n","            #由decoding.call返回的三维decoding_aw\n","            attention_weights = np.reshape(decoding_aw, (-1, 35))\n","            adpre = np.zeros((64,6728))\n","            \n","            for index1,sen in enumerate(inp):\n","                for index2, word_index in enumerate(sen):\n","                #int means only integer can be used as indices\n","                    adpre[index1][int(word_index)] = attention_weights[index1][index2]\n","            #tf.nn.sigmoid之前shape应该是64,1\n","            prob = tf.nn.sigmoid(W11(context_vector) + W22(decoding_hidden) + W33(decoding_input))\n","            #print(prob.shape)\n","            adpre = tf.convert_to_tensor(adpre,tf.float32)\n","            #prob = tf.conver_to_tensor(prob)\n","            #print(prob)\n","            #print(adpre)\n","            new_prediction = (1-prob) * adpre + prob * predictions\n","            loss += loss_function(targ[:, t+1],new_prediction)\n","            \n","    batch_loss = loss / int(targ.shape[0])\n","        \n","    variables = encoder.trainable_variables + decoder.trainable_variables + W11.trainable_variables + W22.trainable_variables + W33.trainable_variables\n","        \n","    gradients = tape.gradient(loss, variables)\n","    optimizer.apply_gradients(zip(gradients, variables))\n","        \n","    return batch_loss   \n","\n","#-------------------------------------------lstm-------------------------\n","def eval_step(inp, targ, encoding_hidden):\n","    loss = 0\n","    \n","    encoding_outputs, encoding_hidden, encoding_c = encoder(inp, encoding_hidden)\n","    inp = inp.numpy()\n","    decoding_hidden = encoding_hidden\n","    decoding_state = encoding_c\n","        #eg: <start> I am here <end>\n","        #1. <start> -> I\n","        #2. I -> am\n","        #3. am -> here\n","        #4. here -> <end>\n","    for t in range(0, targ.shape[1]-1):\n","        decoding_input = tf.expand_dims(targ[:, t],1)\n","        context_vector, predictions, decoding_hidden, decoding_aw, decoding_state = decoder.call(decoding_input, decoding_hidden, encoding_outputs, decoding_state)\n","        attention_weights = np.reshape(decoding_aw, (-1, 35))\n","        adpre = np.zeros((64,6728))\n","        for index1,sen in enumerate(inp):\n","            for index2, word_index in enumerate(sen):\n","                #int means only integer can be used as indices\n","                adpre[index1][int(word_index)] = attention_weights[index1][index2]\n","            #tf.nn.sigmoid之前shape应该是64,1\n","        prob = tf.nn.sigmoid(W11(context_vector) + W22(decoding_hidden) + W33(decoding_input))\n","        #print(prob.shape)\n","        adpre = tf.convert_to_tensor(adpre,tf.float32)\n","        #prob = tf.conver_to_tensor(prob)\n","        new_prediction = (1-prob) * adpre + prob * predictions\n","        loss += loss_function(targ[:, t+1],new_prediction)\n","            \n","    batch_loss = loss / int(targ.shape[0])\n","    return batch_loss "],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"AfvQltoID62n","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":636},"outputId":"938b33e0-d92d-4ab2-e40a-088888fabb02"},"source":["checkpoint_dir = './drive/My Drive/vae_lstm'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n","                                 encoder=encoder,\n","                                 decoder=decoder)\n","adict = {'training_loss': [], 'validation_loss': []}\n","alist = []\n","import time\n","\n","# epochs = 20\n","epochs = 20\n","# steps_per_epoch = len(input_tensor[192:18973]) // batch_size\n","# steps_per_epoch1 = len(input_tensor[18973:]) // batch_size\n","steps_per_epoch = len(input_train) // batch_size\n","steps_per_epoch1 = len(input_eval) // batch_size\n","for epoch in range(epochs):\n","    start = time.time()\n","\n","    encoding_hidden = encoder.initialize_hidden_state()\n","    total_loss = 0\n","    total_loss1 = 0\n","    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n","        # inp_np = inp.numpy()\n","        # print(type(inp_np))\n","        batch_loss = train_step(inp, targ, encoding_hidden)\n","        total_loss += batch_loss\n","\n","        if batch % 100 == 0:\n","            print('Epoch {} Batch {} Loss {:4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n","    if (epoch + 1) % 5 == 0:\n","        checkpoint.save(file_prefix=checkpoint_prefix)\n","\n","\n","    for (batch1, (inp, targ)) in enumerate(eval_dataset.take(steps_per_epoch1)):\n","        eval_loss = eval_step(inp, targ, encoding_hidden)\n","        total_loss1 += eval_loss\n","\n","    print('Epoch {} Loss {:4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n","    print('Epoch {} Eval_Loss {:4f}'.format(epoch + 1, total_loss1 / steps_per_epoch1))\n","    print('Time take for 1 epoch {} sec\\n'.format(time.time() - start))\n","    adict['training_loss'].append(round(float(total_loss / steps_per_epoch), 3))\n","    adict['validation_loss'].append(round(float(total_loss1 / steps_per_epoch1), 3))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1 Batch 0 Loss 10.518854\n","Epoch 1 Batch 100 Loss 5.234774\n","Epoch 1 Batch 200 Loss 4.582544\n","Epoch 1 Loss 5.224816\n","Epoch 1 Eval_Loss 4.309600\n","Time take for 1 epoch 6183.587957859039 sec\n","\n","Epoch 2 Batch 0 Loss 3.802808\n","Epoch 2 Batch 100 Loss 2.758486\n","Epoch 2 Batch 200 Loss 2.632676\n","Epoch 2 Loss 2.904026\n","Epoch 2 Eval_Loss 3.630705\n","Time take for 1 epoch 6249.378209590912 sec\n","\n","Epoch 3 Batch 0 Loss 2.696016\n","Epoch 3 Batch 100 Loss 2.595789\n","Epoch 3 Batch 200 Loss 2.312431\n","Epoch 3 Loss 2.744395\n","Epoch 3 Eval_Loss 3.588391\n","Time take for 1 epoch 6186.506101608276 sec\n","\n","Epoch 4 Batch 0 Loss 2.516091\n","Epoch 4 Batch 100 Loss 2.417343\n","Epoch 4 Batch 200 Loss 2.620786\n","Epoch 4 Loss 2.658012\n","Epoch 4 Eval_Loss 3.556618\n","Time take for 1 epoch 6062.21279835701 sec\n","\n","Epoch 5 Batch 0 Loss 2.784658\n","Epoch 5 Batch 100 Loss 2.681869\n","Epoch 5 Batch 200 Loss 2.412771\n","Epoch 5 Loss 2.608653\n","Epoch 5 Eval_Loss 3.542211\n","Time take for 1 epoch 6125.650223970413 sec\n","\n","Epoch 6 Batch 0 Loss 2.232988\n","Epoch 6 Batch 100 Loss 2.606112\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hy_9MHj9EI7q","colab_type":"code","colab":{}},"source":["print(adict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FR1U9_ioGp0W","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"executionInfo":{"status":"ok","timestamp":1598352625662,"user_tz":-480,"elapsed":13302,"user":{"displayName":"lei ya","photoUrl":"","userId":"05028726856435333308"}},"outputId":"6f6d03a4-9753-47c5-9b2a-ee3d28287103"},"source":["from nltk.corpus import stopwords\n","import tensorflow as tf\n","import matplotlib as mpl\n","import matplotlib.pyplot as  plt\n","import numpy as np\n","import pandas as pd\n","import sklearn\n","import os\n","import sys\n","import time\n","from tensorflow import keras\n","import tensorflow as tf\n","optimizer = keras.optimizers.Adam()\n","checkpoint_dir = './drive/My Drive/vae_lstm'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n","                                 encoder=encoder,\n","                                 decoder=decoder)\n","checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe449a9be48>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"L4KbbxzQEJqT","colab_type":"code","colab":{}},"source":["#Caution: 在不画图的时候一定要将下面两行代码注释掉\n","import matplotlib.ticker as ticker\n","#-------------------lstm----------------\n","def evaluate(input_sentence):\n","    attention_matrix = np.zeros((max_length_output, max_length_input))\n","    \n","    \n","    inputs = [input_tokenizer.word_index[token] for token in input_sentence.split(' ')]\n","    inputs = keras.preprocessing.sequence.pad_sequences(\n","    [inputs], maxlen=max_length_input, padding='post')\n","    \n","    inputs = tf.convert_to_tensor(inputs)\n","    results = ''\n","    #encoding_hidden = encoder.initialize_hidden_state()\n","    #这里的encoding_hidden在lstm情况下需要改一下维度\n","    encoding_hidden = [tf.zeros((1,units)),tf.zeros((1,units))]\n","    \n","    encoding_outputs, encoding_hidden, encoding_c = encoder(inputs, encoding_hidden)\n","    decoding_hidden = encoding_hidden\n","    decoding_state = encoding_c\n","    #eg: <start> I am here <end>\n","        #1. <start> -> I\n","        #2. I -> am\n","        #3. am -> here\n","        #4. here -> <end>\n","    #decoding_inpu.shape :(1,1)(batch_size, length)\n","    decoding_input = tf.expand_dims([output_tokenizer.word_index['<start>']],0)\n","    for t in range(max_length_output):\n","        #attention_weights.shape: (batch_size, input_length, 1)(1,16,1)\n","        context_vector, predictions, decoding_hidden, decoding_aw, decoding_state = decoder(decoding_input, decoding_hidden, encoding_outputs, decoding_state)\n","        \n","         #-------------------------------------------------------------------------\n","        attention_weights = np.reshape(decoding_aw, (-1, 35))\n","        #经历过上一步后attention_weights.shape = 1,35\n","        adpre = np.zeros((1,6728))\n","        for index1,sen in enumerate(inputs):\n","            for index2, word_index in enumerate(sen):\n","                #int means only integer can be used as indices\n","                adpre[index1][int(word_index)] = attention_weights[index1][index2]\n","            #tf.nn.sigmoid之前shape应该是64,1\n","        prob = tf.nn.sigmoid(W11(context_vector) + W22(decoding_hidden) + W33(decoding_input))\n","        #print(prob.shape)\n","        adpre = tf.convert_to_tensor(adpre,tf.float32)\n","        #prob = tf.conver_to_tensor(prob)\n","        new_prediction = (1-prob) * adpre + prob * predictions\n","        #-------------------------------------------------------------------------------\n","        \n","        attention_weights = tf.reshape(decoding_aw, (-1,))#length=16的向量\n","        attention_matrix[t] = attention_weights.numpy()#attention_weights是一个tensor，用numpy（）取出它的值\n","        #predictions.shape: (batch_size, vocab_size)  (1,4935)\n","        \n","        predicted_id = tf.argmax(new_prediction[0]).numpy()\n","        \n","        results += output_tokenizer.index_word[predicted_id] + ' '\n","        if output_tokenizer.index_word[predicted_id] == '<end>':\n","            return results, input_sentence, attention_matrix\n","        \n","        decoding_input = tf.expand_dims([predicted_id],0)\n","    return results, input_sentence, attention_matrix\n","\n","def plot_attention(attention_matrix, input_sentence, predicted_sentence):\n","    fig = plt.figure(figsize=(10,10))\n","    ax = fig.add_subplot(1,1,1)#add a subplot,1,1,1表示子图位置\n","    ax.matshow(attention_matrix, cmap='viridis')#viridis是一种配色方案\n","    \n","    font_dict = {'fontsize': 14}\n","    ax.set_xticklabels([''] + input_sentence, fontdict=font_dict, rotation = 90)#seq2seq里不需要加空格\n","    ax.set_yticklabels([''] + predicted_sentence, fontdict=font_dict)\n","    plt.show()\n","\n","def translate(input_sentence):\n","    results, input_sentence, attention_matrix = evaluate(input_sentence)\n","    \n","    #------------------------\n","    \n","    print('Input: %s' % (input_sentence))\n","    print('Predicted translation: %s' % (results))\n","    \n","    #attention_matrix = attention_matrix[:len(results.split(' ')), :len(input_sentence.split(' '))]\n","    \n","    #plot_attention(attention_matrix, input_sentence.split(' '), results.split(' '))\n","    return results\n","\n","\n","#----------------------BLEU and Meteor------------------------------\n","candidates = []\n","candidates_meteor = []\n","for sample1 in clean_summaries[0:192]:\n","    #res = translate(sample1+', ')\n","    res = translate(sample1)\n","    candidates_meteor.append(res)\n","    candidates.append(res.split(' '))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TCPMlbIsEM6T","colab_type":"code","colab":{}},"source":["from nltk.translate.bleu_score import corpus_bleu\n","score = corpus_bleu(references, candidates,weights=(1,0,0,0))\n","score1 = corpus_bleu(references, candidates,weights=(0.33,0.33,0.33,0))\n","score2 = corpus_bleu(references, candidates,weights=(0.25,0.25,0.25,0.25))\n","print(score,score1,score2)"],"execution_count":null,"outputs":[]}]}