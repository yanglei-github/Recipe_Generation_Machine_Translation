{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nimport tensorflow as tf\nimport matplotlib as mpl\nimport matplotlib.pyplot as  plt\nimport numpy as np\nimport pandas as pd\nimport sklearn\nimport os\nimport sys\nimport time\nfrom tensorflow import keras\n\nimport tensorflow as tf\nembeddings_index = {}\nf = open(\"/kaggle/input/embedding/numberbatch-en-17.04b.txt\")\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = tf.compat.v1.Session(config=config)\n\ndf_clean = pd.read_csv(\"/kaggle/input/total-data/total_data.csv\")\n#---------------------------------------------------------------------------------------\ngiven_ingredient = list(df_clean.ingredients)[0:192]\n#-----------------------------创建评估用词表--------------------------------------------\nreference_recipe = list(df_clean.recipes)\nreference_meteor = []\nreferences = []\nreference_list = reference_recipe[0:192]\nfor astr in reference_list:\n    alist = astr.split(' ')[1:-1]\n    reference_meteor.append(' '.join(alist))\n    references.append([alist])\nclean_summaries = list(df_clean.ingredients)\nclean_texts = list(df_clean.recipes)\nsp_dataset = tuple(clean_summaries)\nen_dataset = tuple(clean_texts)\n#将词语式数据转成ID式\ndef tokenizer(lang):\n    lang_tokenizer = keras.preprocessing.text.Tokenizer(\n        num_words=None, filters='', split=' ')\n    lang_tokenizer.fit_on_texts(lang)#统计词频，生成词表\n    tensor = lang_tokenizer.texts_to_sequences(lang)\n    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding='post')#在句子后面做padding\n    return tensor, lang_tokenizer\n\ninput_tensor, input_tokenizer = tokenizer(sp_dataset)\noutput_tensor, output_tokenizer = tokenizer(en_dataset)\n\ndef max_length(tensor):\n    return max(len(t) for t in tensor)\n\nmax_length_input = max_length(input_tensor)\nmax_length_output = max_length(output_tensor)\nprint(max_length_input, max_length_output)\n\nfrom sklearn.model_selection import train_test_split\ninput_train, input_eval, output_train, output_eval = train_test_split(input_tensor[192:], output_tensor[192:], test_size=0.01, shuffle=False)\n\nbatch_size = 64\nepochs = 20\ndataset = tf.data.Dataset.from_tensor_slices((input_train, output_train))\ntrain_dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder= True)\ntrain_dataset = train_dataset.shuffle(10000)\neval_dataset = tf.data.Dataset.from_tensor_slices((input_eval, output_eval))\neval_dataset = eval_dataset.repeat(epochs).batch(batch_size, drop_remainder= True)\n\nfor x, y in train_dataset.take(1):\n    print(x.shape)\n    print(y.shape)\n    print(x)\n    print(y)\n\nembedding_units = 300\nunits = 300\n#input_tokenizer.word_index是个字典\ninput_vocab_size = len(input_tokenizer.word_index) + 1\noutput_vocab_size = len(output_tokenizer.word_index) + 1\nprint(input_vocab_size,output_vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_DIM = 300\nembedding_matrix = np.zeros((len(input_tokenizer.word_index) + 1, EMBEDDING_DIM))\nfor word, i in input_tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_DIM = 300\nembedding_matrix2 = np.zeros((len(output_tokenizer.word_index) + 1, EMBEDDING_DIM))\nfor word, i in output_tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix2[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-------------------lstm-----------------------------------\nclass Encoder(keras.Model):\n    def __init__(self, vocab_size, embedding_units, encoding_units, batch_size,matrix_of_pretrained_weights):\n        super(Encoder, self).__init__()\n        self.batch_size = batch_size\n        self.encoding_units = encoding_units\n        self.embedding = keras.layers.Embedding(vocab_size, embedding_units,weights=[matrix_of_pretrained_weights],trainable=True)\n        #由于用attention，每步输出需要return_sequences = True\n        self.gru = keras.layers.LSTM(self.encoding_units,\n                                   return_sequences = True,\n                                   return_state = True,\n                                   recurrent_initializer = 'glorot_uniform')\n        \n        \n    #hidden是初始化的隐含状态\n    #这里hidden应该变成[hiddenstate,hiddenc]\n    def call(self, x, hidden):\n        x = self.embedding(x)\n        output, state, state_c = self.gru(x, initial_state = hidden)\n        #output = keras.layers.Dropout(0.75)(output)\n        return output, state, state_c\n    \n    def initialize_hidden_state(self):\n        return [tf.zeros((self.batch_size, self.encoding_units)),tf.zeros((self.batch_size, self.encoding_units))]\n    \nencoder = Encoder(input_vocab_size, embedding_units, units, batch_size,embedding_matrix)\nsample_hidden = encoder.initialize_hidden_state()\nsample_output, sample_hidden, sample_c = encoder.call(x, sample_hidden)\n\nprint('sample_output.shape: ', sample_output.shape)\nprint('sample_hidden.shape: ', sample_hidden.shape)\nprint('sample_memory.shape: ', sample_c.shape)\n\nclass BahdanauAttention(keras.Model):\n    \n    def __init__(self, units):\n        super(BahdanauAttention, self).__init__()\n        self.W1 = keras.layers.Dense(units)\n        self.W2 = keras.layers.Dense(units)\n        self.V = keras.layers.Dense(1)\n        \n    def call(self, decoder_hidden, encoder_outputs):\n        #decoder_hidden.shape = (batch_size, units)\n        #encoder_outputs.shape = (batch_size, length, units)\n        decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, 1)#decoder_hidden.shape = (batch_size, 1, units)\n        #before V:tf.nn.tanh/shape: (batch_size, length, units)\n        #after V:(batch_size, length, 1)\n        score = self.V(\n            tf.nn.tanh(\n                self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time_axis)))\n        \n        #shape: (batch_size, length, 1)\n        attention_weights = tf.nn.softmax(score, axis = 1)\n        #context_vector.shape:(batch_size, length, units)\n        context_vector = attention_weights * encoder_outputs\n        #context_vector.shape:(batch_size, units)\n        context_vector = tf.reduce_sum(context_vector, axis = 1)#在length上求和\n        \n        return context_vector, attention_weights\n\nattention_model = BahdanauAttention(units = 10)#units:经过W1之后的units个数，与batch_size, length, units里units不同\nattention_results, attention_weights = attention_model.call(sample_hidden,sample_output)\n\nprint(attention_results.shape)\nprint(attention_weights.shape)\n\n#---------------------------------lstm-----------------------------------------\nclass Decoder(keras.Model):\n    def __init__(self, vocab_size, embedding_units, decoding_units, batch_size,matrix_of_pretrained_weights):\n        super(Decoder, self).__init__()#调用父类（keras.Model）的构造函数\n        self.batch_size = batch_size\n        self.decoding_units = decoding_units\n        self.embedding = keras.layers.Embedding(vocab_size, embedding_units,weights=[matrix_of_pretrained_weights],trainable=True)\n        self.gru = keras.layers.LSTM(self.decoding_units,\n                                   return_sequences = True,\n                                   return_state = True,\n                                   recurrent_initializer = 'glorot_uniform')\n        \n        self.fc = keras.layers.Dense(vocab_size)\n        self.attention = BahdanauAttention(self.decoding_units)\n        \n        #x:当前步输入，hidden:前一步输出\n    def call(self, x, hidden, encoding_outputs,encoding_state):\n        #context_vector.shape: (bathc_size, units)\n        context_vector, attention_weights = self.attention.call(hidden, encoding_outputs)\n        #befor embedding:x.shape:(batch_size, 1)\n        #after embedding:x.shape:(batch_size, 1, embedding_units)\n        \n        x = self.embedding(x)\n        \n        combined_x = tf.concat([tf.expand_dims(context_vector, 1),x], axis = -1)\n        #output.shape: (batch_size, 1, decoding_units)\n        #state.shape: (batch_size, decoding_units)\n        output, state, state_c = self.gru(combined_x, initial_state = [hidden,encoding_state])\n        #output = keras.layers.Dropout(0.5)(output)\n        #output.shape: (batch_size, decoding_units)\n        output = tf.reshape(output, (-1, output.shape[2]))\n        \n        #output.shape: (batch_size, vocab_size)\n        output = self.fc(output)\n        \n        return context_vector, output, state, attention_weights, state_c\ndecoder = Decoder(output_vocab_size, embedding_units, units, batch_size,embedding_matrix2)\n\noutputs = decoder.call(tf.random.uniform((batch_size,1)), sample_hidden, sample_output,sample_hidden)\n\ncontext_vector, decoder_output, decoder_hidden, decoder_aw, state_c = outputs\n\nprint(decoder_output.shape)\nprint(decoder_hidden.shape)\nprint(decoder_aw.shape)\n\noptimizer = keras.optimizers.Adam()\n\nloss_object = keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = 'none')\n#from_logits直接经过fc的输出没有经过softmax,如果经过softmax就设成False\n\ndef loss_function(real, pred):\n    #输出里的padding不应该计算到损失函数中去\n    mask = tf.math.logical_not(tf.math.equal(real, 0))#是padding时，mask取0\n    loss_ = loss_object(real, pred)\n    mask = tf.cast(mask, dtype = loss_.dtype)\n    loss_ *= mask\n    \n    return tf.reduce_mean(loss_)\n\nW11 = keras.layers.Dense(1)\nW22 = keras.layers.Dense(1)\nW33 = keras.layers.Dense(1)\n\n\n#lstm-----------------------------------------\n#@tf.function#加速cell\ndef train_step(inp, targ, encoding_hidden):\n    loss = 0\n    with tf.GradientTape() as tape:\n        encoding_outputs, encoding_hidden, encoding_c = encoder(inp, encoding_hidden)\n        inp = inp.numpy()\n        decoding_hidden = encoding_hidden\n        decoding_state = encoding_c\n        #eg: <start> I am here <end>\n        #1. <start> -> I\n        #2. I -> am\n        #3. am -> here\n        #4. here -> <end>\n        for t in range(0, targ.shape[1]-1):\n            decoding_input = tf.expand_dims(targ[:, t],1)\n            context_vector, predictions, decoding_hidden, decoding_aw, decoding_state = decoder.call(decoding_input, decoding_hidden, encoding_outputs,decoding_state)\n            #由decoding.call返回的三维decoding_aw\n            attention_weights = np.reshape(decoding_aw, (-1, 35))\n            adpre = np.zeros((64,6728))\n            \n            for index1,sen in enumerate(inp):\n                for index2, word_index in enumerate(sen):\n                #int means only integer can be used as indices\n                    adpre[index1][int(word_index)] = attention_weights[index1][index2]\n            #tf.nn.sigmoid之前shape应该是64,1\n            prob = tf.nn.sigmoid(W11(context_vector) + W22(decoding_hidden) + W33(decoding_input))\n            #print(prob.shape)\n            adpre = tf.convert_to_tensor(adpre,tf.float32)\n            #prob = tf.conver_to_tensor(prob)\n            #print(prob)\n            #print(adpre)\n            new_prediction = (1-prob) * adpre + prob * predictions\n            loss += loss_function(targ[:, t+1],new_prediction)\n            \n    batch_loss = loss / int(targ.shape[0])\n        \n    variables = encoder.trainable_variables + decoder.trainable_variables + W11.trainable_variables + W22.trainable_variables + W33.trainable_variables\n        \n    gradients = tape.gradient(loss, variables)\n    optimizer.apply_gradients(zip(gradients, variables))\n        \n    return batch_loss   \n\n#-------------------------------------------lstm-------------------------\ndef eval_step(inp, targ, encoding_hidden):\n    loss = 0\n    \n    encoding_outputs, encoding_hidden, encoding_c = encoder(inp, encoding_hidden)\n    inp = inp.numpy()\n    decoding_hidden = encoding_hidden\n    decoding_state = encoding_c\n        #eg: <start> I am here <end>\n        #1. <start> -> I\n        #2. I -> am\n        #3. am -> here\n        #4. here -> <end>\n    for t in range(0, targ.shape[1]-1):\n        decoding_input = tf.expand_dims(targ[:, t],1)\n        context_vector, predictions, decoding_hidden, decoding_aw, decoding_state = decoder.call(decoding_input, decoding_hidden, encoding_outputs, decoding_state)\n        attention_weights = np.reshape(decoding_aw, (-1, 35))\n        adpre = np.zeros((64,6728))\n        for index1,sen in enumerate(inp):\n            for index2, word_index in enumerate(sen):\n                #int means only integer can be used as indices\n                adpre[index1][int(word_index)] = attention_weights[index1][index2]\n            #tf.nn.sigmoid之前shape应该是64,1\n        prob = tf.nn.sigmoid(W11(context_vector) + W22(decoding_hidden) + W33(decoding_input))\n        #print(prob.shape)\n        adpre = tf.convert_to_tensor(adpre,tf.float32)\n        #prob = tf.conver_to_tensor(prob)\n        new_prediction = (1-prob) * adpre + prob * predictions\n        loss += loss_function(targ[:, t+1],new_prediction)\n            \n    batch_loss = loss / int(targ.shape[0])\n    return batch_loss ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adict = {'training_loss': [], 'validation_loss': []}\nalist = []\nimport time\n\n# epochs = 20\nepochs = 20\n# steps_per_epoch = len(input_tensor[192:18973]) // batch_size\n# steps_per_epoch1 = len(input_tensor[18973:]) // batch_size\nsteps_per_epoch = len(input_train) // batch_size\nsteps_per_epoch1 = len(input_eval) // batch_size\nfor epoch in range(epochs):\n    start = time.time()\n\n    encoding_hidden = encoder.initialize_hidden_state()\n    total_loss = 0\n    total_loss1 = 0\n    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n        # inp_np = inp.numpy()\n        # print(type(inp_np))\n        batch_loss = train_step(inp, targ, encoding_hidden)\n        total_loss += batch_loss\n\n        if batch % 100 == 0:\n            print('Epoch {} Batch {} Loss {:4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n\n\n    for (batch1, (inp, targ)) in enumerate(eval_dataset.take(steps_per_epoch1)):\n        eval_loss = eval_step(inp, targ, encoding_hidden)\n        total_loss1 += eval_loss\n\n    print('Epoch {} Loss {:4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n    print('Epoch {} Eval_Loss {:4f}'.format(epoch + 1, total_loss1 / steps_per_epoch1))\n    print('Time take for 1 epoch {} sec\\n'.format(time.time() - start))\n    adict['training_loss'].append(round(float(total_loss / steps_per_epoch), 3))\n    adict['validation_loss'].append(round(float(total_loss1 / steps_per_epoch1), 3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(adict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Caution: 在不画图的时候一定要将下面两行代码注释掉\nimport matplotlib.ticker as ticker\n#-------------------lstm----------------\ndef evaluate(input_sentence):\n    attention_matrix = np.zeros((max_length_output, max_length_input))\n    \n    \n    inputs = [input_tokenizer.word_index[token] for token in input_sentence.split(' ')]\n    inputs = keras.preprocessing.sequence.pad_sequences(\n    [inputs], maxlen=max_length_input, padding='post')\n    \n    inputs = tf.convert_to_tensor(inputs)\n    results = ''\n    #encoding_hidden = encoder.initialize_hidden_state()\n    #这里的encoding_hidden在lstm情况下需要改一下维度\n    encoding_hidden = [tf.zeros((1,units)),tf.zeros((1,units))]\n    \n    encoding_outputs, encoding_hidden, encoding_c = encoder(inputs, encoding_hidden)\n    decoding_hidden = encoding_hidden\n    decoding_state = encoding_c\n    #eg: <start> I am here <end>\n        #1. <start> -> I\n        #2. I -> am\n        #3. am -> here\n        #4. here -> <end>\n    #decoding_inpu.shape :(1,1)(batch_size, length)\n    decoding_input = tf.expand_dims([output_tokenizer.word_index['<start>']],0)\n    for t in range(max_length_output):\n        #attention_weights.shape: (batch_size, input_length, 1)(1,16,1)\n        context_vector, predictions, decoding_hidden, decoding_aw, decoding_state = decoder(decoding_input, decoding_hidden, encoding_outputs, decoding_state)\n        \n         #-------------------------------------------------------------------------\n        attention_weights = np.reshape(decoding_aw, (-1, 35))\n        #经历过上一步后attention_weights.shape = 1,35\n        adpre = np.zeros((1,6728))\n        for index1,sen in enumerate(inputs):\n            for index2, word_index in enumerate(sen):\n                #int means only integer can be used as indices\n                adpre[index1][int(word_index)] = attention_weights[index1][index2]\n            #tf.nn.sigmoid之前shape应该是64,1\n        prob = tf.nn.sigmoid(W11(context_vector) + W22(decoding_hidden) + W33(decoding_input))\n        #print(prob.shape)\n        adpre = tf.convert_to_tensor(adpre,tf.float32)\n        #prob = tf.conver_to_tensor(prob)\n        new_prediction = (1-prob) * adpre + prob * predictions\n        #-------------------------------------------------------------------------------\n        \n        attention_weights = tf.reshape(decoding_aw, (-1,))#length=16的向量\n        attention_matrix[t] = attention_weights.numpy()#attention_weights是一个tensor，用numpy（）取出它的值\n        #predictions.shape: (batch_size, vocab_size)  (1,4935)\n        \n        predicted_id = tf.argmax(new_prediction[0]).numpy()\n        \n        results += output_tokenizer.index_word[predicted_id] + ' '\n        if output_tokenizer.index_word[predicted_id] == '<end>':\n            return results, input_sentence, attention_matrix\n        \n        decoding_input = tf.expand_dims([predicted_id],0)\n    return results, input_sentence, attention_matrix\n\ndef plot_attention(attention_matrix, input_sentence, predicted_sentence):\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(1,1,1)#add a subplot,1,1,1表示子图位置\n    ax.matshow(attention_matrix, cmap='viridis')#viridis是一种配色方案\n    \n    font_dict = {'fontsize': 14}\n    ax.set_xticklabels([''] + input_sentence, fontdict=font_dict, rotation = 90)#seq2seq里不需要加空格\n    ax.set_yticklabels([''] + predicted_sentence, fontdict=font_dict)\n    plt.show()\n\ndef translate(input_sentence):\n    results, input_sentence, attention_matrix = evaluate(input_sentence)\n    \n    #------------------------\n    \n    print('Input: %s' % (input_sentence))\n    print('Predicted translation: %s' % (results))\n    \n    #attention_matrix = attention_matrix[:len(results.split(' ')), :len(input_sentence.split(' '))]\n    \n    #plot_attention(attention_matrix, input_sentence.split(' '), results.split(' '))\n    return results\n\n\n#----------------------BLEU and Meteor------------------------------\ncandidates = []\ncandidates_meteor = []\nfor sample1 in clean_summaries[0:192]:\n    #res = translate(sample1+', ')\n    res = translate(sample1)\n    candidates_meteor.append(res)\n    candidates.append(res.split(' '))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\nscore = corpus_bleu(references, candidates,weights=(1,0,0,0))\nscore1 = corpus_bleu(references, candidates,weights=(0.33,0.33,0.33,0))\nscore2 = corpus_bleu(references, candidates,weights=(0.25,0.25,0.25,0.25))\nprint(score,score1,score2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_given_item(given_ingredient, generated_recipe):\n    '''\n    given_ingredient = ['egg tomatoes','chicken']\n    splitted_ingredient = [['egg','tomatoes'],['chicken']]\n    generated_recipe = [['use','egg'],['use','chicken'],...]\n    '''\n\n    total_score = 0\n    extra_score = 0\n    splitted_ingredient = []\n    for bstr in given_ingredient:\n        splitted_ingredient.append(bstr.split(' '))\n\n    for index, each_sample in enumerate(splitted_ingredient):\n        # count1 covered items\n        # count2 extra items\n        count1 = 0\n        count2 = 0\n        for each_word in each_sample:\n            # generated_recipe就是下面的candidates=[['use','egg'],['use','chicken'],...]\n            if each_word in generated_recipe[index]:\n                count1 += 1\n            else:\n                count2 += 1\n        extra_score += count2 / len(each_sample)\n        total_score += count1 / len(each_sample)\n    # -------------------------------------------------------------------------\n    for index, each_recipe in enumerate(generated_recipe):\n        # count1 covered items\n        # count2 extra items\n        extra_item = 0\n        for each_word in each_recipe:\n            # generated_recipe就是下面的candidates=[['use','egg'],['use','chicken'],...]\n            if each_word in input_tokenizer.word_index and each_word not in splitted_ingredient[index]:\n                extra_item += 1\n\n    # --------------------------------------------------------------------------\n\n    total_score = total_score / len(given_ingredient)\n\n    return total_score, extra_item\n\ncover_item, extra_item = calculate_given_item(given_ingredient, candidates)\nprint(cover_item,extra_item)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(reference_meteor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(candidates_meteor)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}